{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7874cfca",
   "metadata": {},
   "source": [
    "# 機械学習の基礎ハンズオン\n",
    "\n",
    "このノートブックでは、機械学習の基礎的な概念と実装方法を学習します。以下のトピックをカバーします：\n",
    "\n",
    "1. 機械学習の概要\n",
    "2. ニューラルネットワークの基本構造\n",
    "3. パラメータ学習の仕組み\n",
    "4. 分類問題と損失関数\n",
    "5. 誤差逆伝播法（バックプロパゲーション）\n",
    "6. 学習率と最適化手法\n",
    "7. 実践：簡単なニューラルネットワークの実装\n",
    "\n",
    "**このハンズオンの目標**：\n",
    "\n",
    "- 機械学習の基本的な考え方を理解する\n",
    "- ニューラルネットワークの仕組みを直感的に把握する\n",
    "- Pythonを使って実際に簡単なモデルを構築・学習させる体験をする\n",
    "- モデルの評価方法や可視化の重要性を学ぶ\n",
    "\n",
    "**前提知識**：\n",
    "\n",
    "基本的なPythonの知識があれば十分ですが、NumPyの基礎を知っているとより理解が深まります。数学的な概念は必要に応じて解説します。\n",
    "\n",
    "それでは、機械学習の世界を一緒に探検していきましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370ad02",
   "metadata": {},
   "source": [
    "## 1. 機械学習の概要\n",
    "\n",
    "機械学習とは、コンピュータがデータから学習し、パターンを見つけ出し、予測や意思決定を行う能力を獲得するための技術です。\n",
    "\n",
    "### なぜ機械学習が重要なのか？\n",
    "\n",
    "従来のプログラミングでは、特定の問題を解決するためのルールを人間がすべて設計する必要がありました。しかし、以下のような場合には従来のアプローチが難しくなります：\n",
    "\n",
    "- ルールが複雑すぎる（例：画像から猫を識別するルール）\n",
    "- ルールが常に変化する（例：スパムメールの特徴）\n",
    "- 個々の状況に合わせたパーソナライズが必要な場合（例：レコメンデーション）\n",
    "\n",
    "機械学習では、「答え」となるデータを与えることで、コンピュータ自身がパターンを学習し、新しいデータに対して予測を行えるようになります。\n",
    "\n",
    "### 機械学習の種類\n",
    "\n",
    "主な学習方法は以下の3つに分類されます：\n",
    "\n",
    "- **教師あり学習**：ラベル付きデータから学習（分類、回帰など）\n",
    "  - 例：メールのスパム判定、住宅価格予測、顔認識\n",
    "  - 入力データと正解（教師データ）のペアからパターンを学習\n",
    "\n",
    "- **教師なし学習**：ラベルなしデータからパターンを発見（クラスタリングなど）\n",
    "  - 例：顧客セグメンテーション、異常検知、次元削減\n",
    "  - 正解ラベルなしで、データ内の構造やグループを自動的に発見\n",
    "\n",
    "- **強化学習**：環境との相互作用から学習（報酬の最大化）\n",
    "  - 例：ゲームAI、ロボット制御、自動運転\n",
    "  - 試行錯誤を通じて、報酬を最大化する行動方針を学習\n",
    "\n",
    "![機械学習の種類](https://i.imgur.com/8mFU5ce.png)\n",
    "\n",
    "今回のハンズオンでは、主に教師あり学習の中でも最も基本的なニューラルネットワークに焦点を当てます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b65731",
   "metadata": {},
   "source": [
    "## 2. 必要なライブラリのインポート\n",
    "\n",
    "まずは、このハンズオンで使用するライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# グラフを表示するための設定\n",
    "%matplotlib inline\n",
    "\n",
    "%pip install -U japanize-matplotlib\n",
    "\n",
    "# 日本語化ライブラリのインポート＆有効化\n",
    "import japanize_matplotlib\n",
    "japanize_matplotlib.japanize()\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# フォントファミリを Meiryo に\n",
    "plt.rcParams['font.family']       = 'Meiryo'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a28e2",
   "metadata": {},
   "source": [
    "## 3. ニューラルネットワークの基本構造\n",
    "\n",
    "ニューラルネットワークは、人間の脳の神経細胞（ニューロン）の仕組みを模倣したモデルです。実際の脳のように、情報は入力から出力へと「層」を通じて伝わっていきます。\n",
    "\n",
    "### ニューラルネットワークの構成要素\n",
    "\n",
    "1. **入力層（Input Layer）**：データの特徴を受け取る層\n",
    "   - 各ニューロンは入力データの1つの特徴量（例：画像のピクセル値、年齢、身長など）に対応\n",
    "   - ニューロン数 = 特徴量の数\n",
    "\n",
    "2. **隠れ層（Hidden Layer）**：入力を変換・処理する中間層\n",
    "   - 複数の層を持つことが可能（これが「深層」学習の「深さ」に相当）\n",
    "   - 複雑なパターンを学習するために重要\n",
    "   - ニューロン数はモデル設計者が決定（ハイパーパーパラメータ）\n",
    "\n",
    "3. **出力層（Output Layer）**：予測・分類結果を出力する層\n",
    "   - 回帰問題：通常1つのニューロン（予測値）\n",
    "   - 分類問題：クラス数に応じたニューロン数（各クラスの確率）\n",
    "\n",
    "4. **重み（Weight）**：層間の接続強度を表すパラメータ\n",
    "   - ネットワークの「知識」を表現\n",
    "   - 学習過程で自動的に調整される\n",
    "\n",
    "5. **バイアス（Bias）**：ニューロンの活性化閾値を調整するパラメータ\n",
    "   - 活性化関数の入力を調整する定数項\n",
    "   - 重みと同様に学習される\n",
    "\n",
    "6. **活性化関数（Activation Function）**：非線形性を導入する関数\n",
    "   - シグモイド：0〜1の範囲の出力（古典的）\n",
    "   - ReLU：max(0, x)（現代的で計算効率が良い）\n",
    "   - tanh：-1〜1の範囲の出力\n",
    "\n",
    "### 数学的な表現\n",
    "\n",
    "一つのニューロンの出力は以下のように計算されます：\n",
    "\n",
    "$$y = f(\\sum_{i=1}^{n} w_i x_i + b)$$\n",
    "\n",
    "ここで：\n",
    "- $x_i$ は入力値\n",
    "- $w_i$ は重み\n",
    "- $b$ はバイアス\n",
    "- $f()$ は活性化関数\n",
    "\n",
    "以下で簡単なニューラルネットワークの構造を視覚化してみましょう。この図を見ることで、ニューロン間の接続や情報の流れについてイメージしやすくなるでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network_structure():\n",
    "    \"\"\"簡単なニューラルネットワークの構造を視覚化する関数\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # 各層のニューロン数\n",
    "    n_input = 3\n",
    "    n_hidden = 4\n",
    "    n_output = 2\n",
    "    \n",
    "    # 各層の位置\n",
    "    layer_positions = [0.2, 0.5, 0.8]  # x座標\n",
    "    \n",
    "    # 入力層\n",
    "    input_neurons = []\n",
    "    for i in range(n_input):\n",
    "        y = (i + 1) / (n_input + 1)\n",
    "        circle = plt.Circle((layer_positions[0], y), 0.05, fill=True, color='lightblue')\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(layer_positions[0] - 0.15, y, f'x{i+1}', fontsize=12)\n",
    "        input_neurons.append((layer_positions[0], y))\n",
    "    \n",
    "    # 隠れ層\n",
    "    hidden_neurons = []\n",
    "    for i in range(n_hidden):\n",
    "        y = (i + 1) / (n_hidden + 1)\n",
    "        circle = plt.Circle((layer_positions[1], y), 0.05, fill=True, color='lightgreen')\n",
    "        ax.add_patch(circle)\n",
    "        hidden_neurons.append((layer_positions[1], y))\n",
    "    \n",
    "    # 出力層\n",
    "    output_neurons = []\n",
    "    for i in range(n_output):\n",
    "        y = (i + 1) / (n_output + 1)\n",
    "        circle = plt.Circle((layer_positions[2], y), 0.05, fill=True, color='salmon')\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(layer_positions[2] + 0.1, y, f'y{i+1}', fontsize=12)\n",
    "        output_neurons.append((layer_positions[2], y))\n",
    "    \n",
    "    # 接続（エッジ）を描画\n",
    "    # 入力層→隠れ層\n",
    "    for i_pos in input_neurons:\n",
    "        for h_pos in hidden_neurons:\n",
    "            ax.plot([i_pos[0], h_pos[0]], [i_pos[1], h_pos[1]], 'k-', alpha=0.3)\n",
    "    \n",
    "    # 隠れ層→出力層\n",
    "    for h_pos in hidden_neurons:\n",
    "        for o_pos in output_neurons:\n",
    "            ax.plot([h_pos[0], o_pos[0]], [h_pos[1], o_pos[1]], 'k-', alpha=0.3)\n",
    "    \n",
    "    # レイヤーラベル\n",
    "    ax.text(layer_positions[0], 0.05, '入力層', ha='center', fontsize=14)\n",
    "    ax.text(layer_positions[1], 0.05, '隠れ層', ha='center', fontsize=14)\n",
    "    ax.text(layer_positions[2], 0.05, '出力層', ha='center', fontsize=14)\n",
    "    \n",
    "    # 数式の例\n",
    "    ax.text(0.5, 0.95, r'隠れ層: $h_j = f(\\sum_{i} w_{ij}x_i + b_j)$', ha='center', fontsize=12)\n",
    "    ax.text(0.5, 0.9, r'出力層: $y_k = g(\\sum_{j} w_{jk}h_j + b_k)$', ha='center', fontsize=12)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ニューラルネットワークの図を表示\n",
    "visualize_network_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf858865",
   "metadata": {},
   "source": [
    "### 活性化関数\n",
    "\n",
    "活性化関数はニューラルネットワークに非線形性を導入する重要な要素です。これがなければ、どれだけ層を重ねても単なる線形変換にしかならず、複雑なパターンを学習できません。\n",
    "\n",
    "#### なぜ活性化関数が必要なのか？\n",
    "\n",
    "活性化関数がない場合、ネットワークは以下のようになります：\n",
    "\n",
    "$$y = W_2(W_1x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)$$\n",
    "\n",
    "これは単なる線形関数であり、複数の層を重ねる意味がありません。活性化関数を導入することで、ネットワークは複雑な非線形関係を学習できるようになります。\n",
    "\n",
    "#### 代表的な活性化関数とその特徴\n",
    "\n",
    "1. **シグモイド関数 (Sigmoid)**:\n",
    "   - 出力範囲：0〜1\n",
    "   - 特徴：古典的によく使われたが、勾配消失問題がある\n",
    "   - 用途：二値分類の出力層などに使用\n",
    "\n",
    "2. **双曲線正接関数 (tanh)**:\n",
    "   - 出力範囲：-1〜1\n",
    "   - 特徴：シグモイドと同様の形状だが、原点中心\n",
    "   - 用途：隠れ層に使用されることがある\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit)**:\n",
    "   - 定義：$f(x) = max(0, x)$\n",
    "   - 特徴：計算が単純、勾配消失問題が少ない\n",
    "   - 問題点：「死んだReLU」問題（学習中にニューロンが活性化しなくなる）\n",
    "   - 用途：現代のニューラルネットワークの隠れ層で最も一般的\n",
    "\n",
    "4. **Leaky ReLU**:\n",
    "   - 定義：$f(x) = max(\\alpha x, x)$ （$\\alpha$は小さな正の値、例えば0.01）\n",
    "   - 特徴：ReLUの改良版で「死んだReLU」問題を軽減\n",
    "\n",
    "以下では、これらの活性化関数の形状をグラフで確認し、その特性を視覚的に理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f361d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_functions():\n",
    "    \"\"\"代表的な活性化関数をプロットする関数\"\"\"\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    \n",
    "    # 各活性化関数の定義\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def leaky_relu(x, alpha=0.1):\n",
    "        return np.maximum(alpha * x, x)\n",
    "    \n",
    "    # グラフ描画\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Sigmoid\n",
    "    axs[0].plot(x, sigmoid(x), 'b-', linewidth=2)\n",
    "    axs[0].set_title('シグモイド関数 (Sigmoid)')\n",
    "    axs[0].set_xlabel('入力')\n",
    "    axs[0].set_ylabel('出力')\n",
    "    axs[0].grid(True)\n",
    "    axs[0].text(1, 0.2, r'$f(x) = \\frac{1}{1 + e^{-x}}$', fontsize=12)\n",
    "    \n",
    "    # tanh\n",
    "    axs[1].plot(x, tanh(x), 'g-', linewidth=2)\n",
    "    axs[1].set_title('双曲線正接関数 (tanh)')\n",
    "    axs[1].set_xlabel('入力')\n",
    "    axs[1].set_ylabel('出力')\n",
    "    axs[1].grid(True)\n",
    "    axs[1].text(1, 0.2, r'$f(x) = \\tanh(x)$', fontsize=12)\n",
    "    \n",
    "    # ReLU\n",
    "    axs[2].plot(x, relu(x), 'r-', linewidth=2)\n",
    "    axs[2].set_title('正規化線形関数 (ReLU)')\n",
    "    axs[2].set_xlabel('入力')\n",
    "    axs[2].set_ylabel('出力')\n",
    "    axs[2].grid(True)\n",
    "    axs[2].text(1, 2, r'$f(x) = \\max(0, x)$', fontsize=12)\n",
    "    \n",
    "    # Leaky ReLU\n",
    "    axs[3].plot(x, leaky_relu(x), 'm-', linewidth=2)\n",
    "    axs[3].set_title('リーキー正規化線形関数 (Leaky ReLU)')\n",
    "    axs[3].set_xlabel('入力')\n",
    "    axs[3].set_ylabel('出力')\n",
    "    axs[3].grid(True)\n",
    "    axs[3].text(1, 2, r'$f(x) = \\max(\\alpha x, x), \\alpha=0.1$', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 活性化関数のグラフを表示\n",
    "plot_activation_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3372389",
   "metadata": {},
   "source": [
    "## 4. パラメータ学習の仕組み\n",
    "\n",
    "ニューラルネットワークの学習過程は以下のステップで行われます：\n",
    "\n",
    "1. **順伝播（Forward Propagation）**：入力データがネットワークを通過し、予測値を生成\n",
    "2. **損失計算**：予測値と実際の値（正解ラベル）との誤差を計算\n",
    "3. **逆伝播（Back Propagation）**：誤差を用いて各パラメータの勾配を計算\n",
    "4. **パラメータ更新**：勾配を使って重みとバイアスを更新（最適化アルゴリズム）\n",
    "\n",
    "### 重要な概念：エポック（Epoch）\n",
    "\n",
    "**エポック**とは、機械学習において全ての訓練データが一度ネットワークを通過することを指します。つまり、1エポックとは訓練データセット全体を1回学習することです。例えば、1000件のデータがあるとき、1000件全てを使って学習を1回行うと「1エポック完了」となります。\n",
    "\n",
    "多くの場合、ニューラルネットワークはデータを何度も繰り返し学習することで精度を向上させます。10エポック、100エポック、1000エポックといった具合に複数回の学習を行います。エポック数はハイパーパラメータの一つで、少なすぎると「過少学習」（モデルが十分に学習できない）、多すぎると「過学習」（訓練データに特化しすぎて汎化性能が下がる）の原因になることがあります。\n",
    "\n",
    "このエポックという概念は後述する学習曲線（損失や精度のグラフ）を理解する上で重要です。\n",
    "\n",
    "以下では、簡単な例を通して学習アルゴリズムを実装していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b796272",
   "metadata": {},
   "source": [
    "### データセットの準備\n",
    "\n",
    "まずは簡単な分類問題のためのデータセットを用意します。scikit-learnのIrisデータセットを使用して、花の種類を分類するモデルを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cdc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Irisデータセットの読み込み\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 簡単のため、2種類の花だけを使用\n",
    "X = X[y < 2]\n",
    "y = y[y < 2]\n",
    "\n",
    "# データの標準化\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# トレーニングセットとテストセットに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"データの形状: {X.shape}\")\n",
    "print(f\"クラスの分布: {np.bincount(y)}\")\n",
    "\n",
    "# データを可視化する\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, feature_idx in enumerate([(0, 1), (2, 3)]):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    for label in range(2):\n",
    "        plt.scatter(X[y == label, feature_idx[0]], X[y == label, feature_idx[1]], \n",
    "                   label=f'Class {label}')\n",
    "    plt.xlabel(f'Feature {feature_idx[0]}')\n",
    "    plt.ylabel(f'Feature {feature_idx[1]}')\n",
    "    plt.legend()\n",
    "    plt.title(f'Iris Features {feature_idx[0]} vs {feature_idx[1]}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee77f50",
   "metadata": {},
   "source": [
    "## 5. シンプルなニューラルネットワークの実装\n",
    "\n",
    "ここでは、入力層（4特徴）、隠れ層（5ユニット）、出力層（1ユニット）から成る簡単なニューラルネットワークを実装します。\n",
    "\n",
    "### コードの解説\n",
    "\n",
    "これから実装するニューラルネットワークの各パートについて説明します：\n",
    "\n",
    "1. **初期化（`__init__`メソッド）**：\n",
    "   - ネットワークの重みとバイアスを初期化します\n",
    "   - Xavier初期化を使用して重みを設定（学習の安定化に重要）\n",
    "   - 損失と精度の履歴を記録するためのリストを準備\n",
    "\n",
    "2. **活性化関数と導関数**：\n",
    "   - シグモイド関数：出力を0〜1の範囲に変換\n",
    "   - シグモイド導関数：逆伝播時に勾配計算に使用\n",
    "\n",
    "3. **順伝播（`forward`メソッド）**：\n",
    "   - 入力から隠れ層への変換: $z_1 = XW_1 + b_1$\n",
    "   - 隠れ層の活性化: $a_1 = \\sigma(z_1)$\n",
    "   - 隠れ層から出力層への変換: $z_2 = a_1W_2 + b_2$\n",
    "   - 出力層の活性化: $a_2 = \\sigma(z_2)$\n",
    "\n",
    "4. **損失計算（`compute_loss`メソッド）**：\n",
    "   - 二値交差エントロピー損失：$L = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$\n",
    "   - 予測値と実際の値の差を数値化する指標\n",
    "\n",
    "5. **逆伝播（`backward`メソッド）**：\n",
    "   - 出力層の誤差を計算: $dz_2 = a_2 - y$\n",
    "   - 出力層の重みとバイアスの勾配を計算\n",
    "   - 隠れ層の誤差を計算: $dz_1 = dz_2W_2^T \\odot \\sigma'(a_1)$\n",
    "   - 隠れ層の重みとバイアスの勾配を計算\n",
    "   - 勾配を使って重みとバイアスを更新\n",
    "\n",
    "6. **予測と評価（`predict`と`accuracy`メソッド）**：\n",
    "   - 予測：出力が0.5以上なら1、そうでなければ0\n",
    "   - 精度：正確に分類されたサンプルの割合\n",
    "\n",
    "7. **学習プロセス（`fit`メソッド）**：\n",
    "   - 指定されたエポック数だけ学習を繰り返す\n",
    "   - 各エポックで損失と精度を計算して記録\n",
    "   - 学習の進行状況を定期的に表示\n",
    "\n",
    "以下でこのネットワークを実装していきます。コード内のコメントも参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425738fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"ニューラルネットワークの初期化\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            入力層のニューロン数（特徴量の数）\n",
    "        hidden_size : int\n",
    "            隠れ層のニューロン数\n",
    "        output_size : int\n",
    "            出力層のニューロン数（二値分類の場合は1）\n",
    "        \n",
    "        Notes:\n",
    "        ------\n",
    "        Xavier初期化を使って重みを初期化しています。これは学習の収束を早めるために重要です。\n",
    "        \"\"\"\n",
    "        # Xavier初期化で重みを初期化 - 初期値が重要！\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "        \n",
    "        # 学習履歴の保存用\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"シグモイド活性化関数\n",
    "        入力値を0〜1の範囲に変換します\n",
    "        \n",
    "        入力値が大きすぎる場合にオーバーフローを防ぐため、クリッピングを行っています。\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # オーバーフローを防止\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"シグモイド関数の導関数\n",
    "        逆伝播時の勾配計算に使用します\n",
    "        \n",
    "        注意：入力xはすでにシグモイド関数を通過した値を想定しています\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播（フォワードプロパゲーション）\n",
    "        入力データがネットワークを通過して出力を生成するプロセスです\n",
    "        \"\"\"\n",
    "        # 隠れ層の計算: z1 = X・W1 + b1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1  # 線形変換\n",
    "        self.a1 = self.sigmoid(self.z1)  # 活性化（非線形変換）\n",
    "        \n",
    "        # 出力層の計算: z2 = a1・W2 + b2\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # 線形変換\n",
    "        self.a2 = self.sigmoid(self.z2)  # 活性化（非線形変換）\n",
    "        \n",
    "        return self.a2  # 最終出力（0〜1の確率値）\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"二値交差エントロピー損失関数\n",
    "        予測値と実際の値の誤差を計算します\n",
    "        二値分類問題で広く使われる損失関数です\n",
    "        \"\"\"\n",
    "        # 数値安定性のためのクリッピング（0や1に完全に近いと計算が不安定になるため）\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        # 二値交差エントロピー: -(y*log(p) + (1-y)*log(1-p))\n",
    "        return - np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"逆伝播（バックプロパゲーション）と重みの更新\n",
    "        損失関数の勾配を計算し、重みを更新します\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # サンプル数（バッチサイズ）\n",
    "        \n",
    "        # 出力層の誤差: dz2 = a2 - y (二値交差エントロピーと活性化関数がシグモイドの場合の簡略化式)\n",
    "        dz2 = self.a2 - y.reshape(-1, 1)  # 予測値と実際の値の差\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m  # 出力層の重みに関する勾配\n",
    "        db2 = np.sum(dz2, axis=0) / m  # 出力層のバイアスに関する勾配\n",
    "        \n",
    "        # 隠れ層の誤差: dz1 = dz2・W2^T * sigmoid_derivative(a1)\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.a1)  # 誤差の逆伝播\n",
    "        dW1 = np.dot(X.T, dz1) / m  # 隠れ層の重みに関する勾配\n",
    "        db1 = np.sum(dz1, axis=0) / m  # 隠れ層のバイアスに関する勾配\n",
    "        \n",
    "        # 勾配降下法による重みとバイアスの更新: θ = θ - α・∇θ\n",
    "        self.W2 -= learning_rate * dW2  # 出力層の重みを更新\n",
    "        self.b2 -= learning_rate * db2  # 出力層のバイアスを更新\n",
    "        self.W1 -= learning_rate * dW1  # 隠れ層の重みを更新\n",
    "        self.b1 -= learning_rate * db1  # 隠れ層のバイアスを更新\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"予測を行う関数\n",
    "        確率値が0.5以上なら1、そうでなければ0と判定します（二値分類）\n",
    "        \"\"\"\n",
    "        return (self.forward(X) >= 0.5).astype(int)  # 0.5を閾値として0か1かを判定\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"モデルの精度を計算する関数\n",
    "        正確に分類できたサンプルの割合を返します\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions.flatten() == y)  # 予測が正解と一致する割合\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "        \"\"\"モデルの学習を行う関数\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            訓練データの特徴量\n",
    "        y : numpy.ndarray\n",
    "            訓練データの正解ラベル\n",
    "        epochs : int\n",
    "            学習を繰り返す回数\n",
    "        learning_rate : float\n",
    "            学習率（パラメータ更新の大きさを制御）\n",
    "        verbose : bool\n",
    "            学習の途中経過を表示するかどうか\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # 順伝播: 予測値を計算\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # 損失と精度を計算\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            acc = self.accuracy(X, y)\n",
    "            \n",
    "            # 学習履歴を保存\n",
    "            self.loss_history.append(loss)\n",
    "            self.accuracy_history.append(acc)\n",
    "            \n",
    "            # 逆伝播と重みの更新\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            # 途中経過の表示（100エポックごと、または最後のエポックで表示）\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs-1):\n",
    "                print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# モデルの設定パラメータを準備\n",
    "input_size = X_train.shape[1]  # 入力層のサイズ（特徴量の数 = 4）\n",
    "hidden_size = 5  # 隠れ層のサイズ（ハイパーパラメータとして調整可能）\n",
    "output_size = 1  # 出力層のサイズ（バイナリ分類なので1）\n",
    "\n",
    "# ニューラルネットワークのインスタンスを作成\n",
    "print(f\"ネットワーク構成: 入力層({input_size}) → 隠れ層({hidden_size}) → 出力層({output_size})\")\n",
    "nn = SimpleNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# モデルの学習を開始（学習率は0.1、1000エポック）\n",
    "print(\"学習を開始します...\")\n",
    "nn.fit(X_train, y_train, epochs=1000, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671da007",
   "metadata": {},
   "source": [
    "## 6. モデルの評価と可視化\n",
    "\n",
    "学習したモデルをテストデータで評価し、学習過程を可視化します。\n",
    "\n",
    "### 学習曲線について\n",
    "\n",
    "**学習曲線（Learning Curve）** は、モデルの学習過程を視覚化したグラフです。横軸はエポック数（学習の繰り返し回数）、縦軸は損失（Loss）や精度（Accuracy）などの評価指標を表します。\n",
    "\n",
    "- **損失曲線**: 理想的には、エポックが進むにつれて徐々に減少し、最終的に低い値で安定します。急激な減少や不安定な動きは、学習率が高すぎることを示唆する場合があります。\n",
    "\n",
    "- **精度曲線**: エポックが進むにつれて増加し、高い値で安定するのが理想的です。\n",
    "\n",
    "これらのグラフを観察することで、以下のような問題を診断できます：\n",
    "- **過学習（Overfitting）**: 訓練データの精度は高いがテストデータの精度が低い場合\n",
    "- **過少学習（Underfitting）**: 訓練データもテストデータも精度が低い場合\n",
    "- **最適な学習回数**: 精度が最大に達するエポック数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの精度評価\n",
    "test_accuracy = nn.accuracy(X_test, y_test)\n",
    "print(f\"テストデータでの精度: {test_accuracy:.4f}\")\n",
    "\n",
    "# 学習過程の可視化\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 損失のグラフ\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nn.loss_history, 'b-')\n",
    "plt.xlabel('エポック')\n",
    "plt.ylabel('損失')\n",
    "plt.title('学習曲線（損失）')\n",
    "plt.grid(True)\n",
    "\n",
    "# 精度のグラフ\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(nn.accuracy_history, 'r-')\n",
    "plt.xlabel('エポック')\n",
    "plt.ylabel('精度')\n",
    "plt.title('学習曲線（精度）')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67943cad",
   "metadata": {},
   "source": [
    "## 7. 決定境界の可視化\n",
    "\n",
    "### 決定境界とは何か\n",
    "\n",
    "**決定境界（Decision Boundary）** とは、異なるクラスを分類する境界線や境界面のことです。分類問題において、モデルはデータ空間を複数の領域に分割し、それぞれの領域に特定のクラスラベルを割り当てます。その領域間の境界が決定境界です。\n",
    "\n",
    "2クラス分類の場合、決定境界は以下のように定義できます：\n",
    "- 入力されたデータがクラス0とクラス1のどちらに属するかを判断する境界線\n",
    "- 数学的には、モデルの出力確率が0.5となる点の集合（二値分類の場合）\n",
    "\n",
    "決定境界の形状はモデルの複雑さによって異なります：\n",
    "- **線形モデル**：直線や平面の決定境界\n",
    "- **非線形モデル**（ニューラルネットワークなど）：曲線や複雑な形状の決定境界\n",
    "\n",
    "決定境界の可視化は、モデルの挙動を理解するための重要なツールであり、特に以下の点で役立ちます：\n",
    "- モデルがどのようにデータを分類しているかの直感的な理解\n",
    "- モデルの複雑さが適切かどうかの評価（単純すぎる/複雑すぎる）\n",
    "- 誤分類されるデータポイントのパターンの特定\n",
    "\n",
    "以下では、2次元の特徴量を選択して、学習したモデルの決定境界を可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29148181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, features=(0, 1)):\n",
    "    \"\"\"決定境界を可視化する関数\"\"\"\n",
    "    # 特徴量を選択\n",
    "    X_subset = X[:, features]\n",
    "    \n",
    "    # メッシュグリッドを作成\n",
    "    h = 0.02  # グリッドの粒度\n",
    "    x_min, x_max = X_subset[:, 0].min() - 1, X_subset[:, 0].max() + 1\n",
    "    y_min, y_max = X_subset[:, 1].min() - 1, X_subset[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # グリッドポイントの予測\n",
    "    # まず、全特徴量を含むデータフレームを作成\n",
    "    grid = np.ones((xx.ravel().shape[0], X.shape[1])) * np.mean(X, axis=0)\n",
    "    grid[:, features[0]] = xx.ravel()\n",
    "    grid[:, features[1]] = yy.ravel()\n",
    "    \n",
    "    # 予測\n",
    "    Z = model.predict(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # 決定境界をプロット\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)\n",
    "    plt.scatter(X_subset[:, 0], X_subset[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    plt.xlabel(f'特徴量 {features[0]}')\n",
    "    plt.ylabel(f'特徴量 {features[1]}')\n",
    "    plt.title('決定境界と訓練データ')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 決定境界の可視化（特徴量の組み合わせを2つ試してみる）\n",
    "print(\"特徴量0と1を使った決定境界の可視化：\")\n",
    "print(\"この図は、モデルが特徴量0と1の値に基づいてどのように分類決定を下すかを示しています。\")\n",
    "print(\"色の濃い領域がクラス1、薄い領域がクラス0と予測される領域です。点は実際のデータポイントです。\")\n",
    "plot_decision_boundary(nn, X, y, features=(0, 1))\n",
    "\n",
    "print(\"特徴量2と3を使った決定境界の可視化：\")\n",
    "print(\"この図からは、モデルが別の特徴量の組み合わせでどのように分類するかがわかります。\")\n",
    "print(\"決定境界の形状の複雑さから、モデルの表現力を確認できます。\")\n",
    "plot_decision_boundary(nn, X, y, features=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a496a",
   "metadata": {},
   "source": [
    "### 決定境界の解釈\n",
    "\n",
    "上記の図を見ると、モデルがどのように特徴空間を分割しているかがわかります。以下の点に注目してみましょう：\n",
    "\n",
    "1. **境界の形状**：\n",
    "   - 直線的な境界は単純なモデルを示す\n",
    "   - 曲線的な境界は複雑なモデルを示す\n",
    "   - 本例では、簡単なニューラルネットワークでも非線形の決定境界を学習できています\n",
    "\n",
    "2. **誤分類されたデータポイント**：\n",
    "   - 決定境界の「間違った側」にあるデータポイントは誤分類されています\n",
    "   - これらのポイントを観察することで、モデルが苦手とするパターンを把握できます\n",
    "\n",
    "3. **分類の確信度**：\n",
    "   - 決定境界から離れたデータポイントほど、分類の確信度が高いと考えられます\n",
    "   - 境界付近のデータは分類が不確かで、小さな特徴変化で分類結果が変わる可能性があります\n",
    "\n",
    "4. **特徴量の重要度**：\n",
    "   - 異なる特徴量の組み合わせで決定境界の形状が大きく異なる場合、特徴量の重要度に差があることを示唆します\n",
    "   - よりきれいに分離できる特徴量の組み合わせは、分類により有用である可能性が高いです\n",
    "\n",
    "決定境界の可視化は、モデルの内部動作を「ブラックボックス」から「理解可能なもの」へと変える強力なツールです。実際の応用では、このような可視化によってモデルの弱点を特定し、改善することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082274e",
   "metadata": {},
   "source": [
    "## 8. 学習率の影響\n",
    "\n",
    "### 学習率とは\n",
    "\n",
    "**学習率（Learning Rate）** は、勾配降下法においてパラメータを更新する際の「ステップサイズ」を制御するハイパーパラメータです。簡単に言えば、各反復でモデルパラメータをどれだけ調整するかを決める「速度調整つまみ」のようなものです。\n",
    "\n",
    "数学的には、パラメータの更新式は以下のようになります：\n",
    "\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla J(\\theta)$$\n",
    "\n",
    "ここで、$\\eta$（イータ）が学習率、$\\nabla J(\\theta)$が損失関数の勾配です。\n",
    "\n",
    "学習率の選択は非常に重要で、以下のような影響があります：\n",
    "\n",
    "- **学習率が大きすぎる場合**：\n",
    "  - 最適値を「飛び越えてしまう」可能性がある\n",
    "  - 収束せずに発散する可能性がある\n",
    "  - 学習が不安定になる（損失のグラフが振動する）\n",
    "\n",
    "- **学習率が小さすぎる場合**：\n",
    "  - 学習が非常に遅くなる\n",
    "  - 局所的最適解に留まる可能性が高くなる\n",
    "  - 収束に必要なエポック数が大幅に増加する\n",
    "\n",
    "最適な学習率は問題やデータセットによって異なるため、しばしば実験的に決定されます。また、学習過程で学習率を動的に調整する手法（学習率スケジューリング）も広く使われています。\n",
    "\n",
    "以下では、異なる学習率を用いた場合の学習過程の違いを見ていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_learning_rates(X_train, y_train, learning_rates=[0.001, 0.01, 0.1, 1.0], epochs=1000):\n",
    "    \"\"\"異なる学習率での学習過程を比較する関数\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 学習率ごとに異なる色を使用\n",
    "    colors = ['blue', 'green', 'red', 'purple']\n",
    "    \n",
    "    for i, lr in enumerate(learning_rates):\n",
    "        # モデルの作成と学習\n",
    "        print(f\"学習率 {lr} でモデルを学習中...\")\n",
    "        model = SimpleNeuralNetwork(input_size, hidden_size, output_size)\n",
    "        model.fit(X_train, y_train, epochs=epochs, learning_rate=lr, verbose=False)\n",
    "        \n",
    "        # テストデータでの精度を評価\n",
    "        test_acc = model.accuracy(X_test, y_test)\n",
    "        print(f\"学習率 {lr} のテスト精度: {test_acc:.4f}\")\n",
    "        \n",
    "        # 損失のグラフ (全体)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(model.loss_history, label=f'lr={lr}', color=colors[i])\n",
    "        plt.xlabel('エポック')\n",
    "        plt.ylabel('損失')\n",
    "        plt.title('学習率による損失の変化（全エポック）')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 精度のグラフ (全体)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(model.accuracy_history, label=f'lr={lr}', color=colors[i])\n",
    "        plt.xlabel('エポック')\n",
    "        plt.ylabel('精度')\n",
    "        plt.title('学習率による精度の変化（全エポック）')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 初期の損失のグラフ（最初の100エポック）\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(model.loss_history[:100], label=f'lr={lr}', color=colors[i])\n",
    "        plt.xlabel('エポック')\n",
    "        plt.ylabel('損失')\n",
    "        plt.title('学習率による初期の損失変化（最初の100エポック）')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 初期の精度のグラフ（最初の100エポック）\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(model.accuracy_history[:100], label=f'lr={lr}', color=colors[i])\n",
    "        plt.xlabel('エポック')\n",
    "        plt.ylabel('精度')\n",
    "        plt.title('学習率による初期の精度変化（最初の100エポック）')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 異なる学習率での学習過程を比較\n",
    "print(\"異なる学習率がニューラルネットワークの学習にどのような影響を与えるか観察します。\")\n",
    "print(\"学習率が大きいと素早く学習するが不安定になることがあり、小さいと安定するが学習が遅くなります。\")\n",
    "compare_learning_rates(X_train, y_train, learning_rates=[0.001, 0.01, 0.1, 1.0], epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29f683",
   "metadata": {},
   "source": [
    "### 学習率の違いによる影響の考察\n",
    "\n",
    "上記のグラフから、学習率がモデルの学習に大きく影響することがわかります：\n",
    "\n",
    "1. **小さすぎる学習率（0.001）** の場合：\n",
    "   - 損失の減少が非常に緩やか（青線）\n",
    "   - 収束に多くのエポックが必要\n",
    "   - 学習が遅く、1000エポックでも最適解に到達していない可能性がある\n",
    "\n",
    "2. **適切な学習率（0.01〜0.1）** の場合：\n",
    "   - 損失が順調に減少し、適切な速さで収束する（緑・赤線）\n",
    "   - 精度も安定して上昇する\n",
    "   - 適度なスピードで最適解に近づいていく\n",
    "\n",
    "3. **大きすぎる学習率（1.0）** の場合：\n",
    "   - 初期段階で急速に学習する可能性がある（紫線）\n",
    "   - しかし、損失や精度のグラフが振動し、不安定になる可能性がある\n",
    "   - 最適解を「飛び越えて」収束しない場合もある\n",
    "\n",
    "#### 実際のプロジェクトでの学習率の選び方\n",
    "\n",
    "1. **グリッドサーチ（Grid Search）**：\n",
    "   - 複数の学習率（例：0.0001, 0.001, 0.01, 0.1）を試して最良のものを選ぶ\n",
    "\n",
    "2. **学習率スケジューリング**：\n",
    "   - 学習の初期段階では大きい学習率（素早く大まかな最適解の領域に到達）\n",
    "   - 後半では小さい学習率（細かく最適解を調整）に減衰させる\n",
    "\n",
    "3. **適応的学習率**：\n",
    "   - Adam, RMSprop などの最適化アルゴリズムは、パラメータごとに学習率を自動調整する\n",
    "   - 実務では、これらの高度な最適化手法がよく使われる\n",
    "\n",
    "最適な学習率は問題やデータセットごとに異なるため、実験的に探索することが重要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53121a",
   "metadata": {},
   "source": [
    "## 9. まとめと応用\n",
    "\n",
    "このハンズオンでは、機械学習の基礎について学びました：\n",
    "\n",
    "1. **ニューラルネットワークの基本構造**：入力層、隠れ層、出力層からなる構造\n",
    "2. **活性化関数**：シグモイド、tanh、ReLU、Leaky ReLUなどの非線形変換\n",
    "3. **パラメータ学習**：順伝播と逆伝播による重みの更新\n",
    "4. **損失関数**：予測と実際の値の誤差を測定する関数\n",
    "5. **勾配降下法**：損失関数の勾配を使ったパラメータの最適化\n",
    "6. **学習率**：パラメータ更新の幅を制御するハイパーパラメータ\n",
    "\n",
    "### 次のステップとして学ぶべきこと\n",
    "\n",
    "1. **深層ニューラルネットワーク**：より多くの隠れ層を持つモデル\n",
    "2. **正則化テクニック**：過学習を防ぐためのテクニック（ドロップアウト、L1/L2正則化など）\n",
    "3. **高度な最適化アルゴリズム**：Adam, RMSprop, Momentum SGDなど\n",
    "4. **畳み込みニューラルネットワーク（CNN）**：画像処理に特化したアーキテクチャ\n",
    "5. **再帰型ニューラルネットワーク（RNN）**：系列データ処理に特化したアーキテクチャ\n",
    "\n",
    "以上のトピックを深く理解することで、より高度な機械学習モデルを構築する基盤が築かれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6964e1",
   "metadata": {},
   "source": [
    "## 10. 学習リソースと実践的なヒント\n",
    "\n",
    "### 機械学習をさらに学ぶためのリソース\n",
    "\n",
    "このハンズオンで学んだ基礎知識をさらに深めるために、以下のリソースがおすすめです：\n",
    "\n",
    "#### オンラインコース\n",
    "\n",
    "1. **[Coursera - Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning)**\n",
    "   - 機械学習の基礎から応用までを網羅した定番コース\n",
    "\n",
    "2. **[fast.ai](https://www.fast.ai/)**\n",
    "   - 実践的なアプローチで深層学習を学べるコース\n",
    "\n",
    "3. **[Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)**\n",
    "   - ニューラルネットワークの理論と実装を深く学べる\n",
    "\n",
    "#### 書籍\n",
    "\n",
    "1. **『ゼロから作るDeep Learning』**（斎藤康毅著）\n",
    "   - Pythonで一からニューラルネットワークを実装する過程を通して学べる\n",
    "\n",
    "2. **『はじめてのパターン認識』**（平井有三著）\n",
    "   - パターン認識と機械学習の基礎を丁寧に解説\n",
    "\n",
    "3. **『Deep Learning』**（Ian Goodfellow, Yoshua Bengio, Aaron Courville著）\n",
    "   - 深層学習の理論を詳細に学びたい方向け\n",
    "\n",
    "#### 実践的なコードリポジトリ\n",
    "\n",
    "1. **[scikit-learn Examples](https://scikit-learn.org/stable/auto_examples/index.html)**\n",
    "   - 様々な機械学習アルゴリズムの使用例\n",
    "\n",
    "2. **[TensorFlow Tutorials](https://www.tensorflow.org/tutorials)**\n",
    "   - TensorFlowを使った深層学習モデルの構築方法\n",
    "\n",
    "3. **[PyTorch Tutorials](https://pytorch.org/tutorials/)**\n",
    "   - PyTorchの基礎から応用までのチュートリアル\n",
    "\n",
    "### 実践的なヒント\n",
    "\n",
    "1. **小さなプロジェクトから始める**\n",
    "   - 大きなプロジェクトよりも、小さな問題を解くことから始めると理解が深まります\n",
    "   - Kaggleの初心者向けコンペティション（Titanic, House Pricesなど）に挑戦してみましょう\n",
    "\n",
    "2. **データの前処理と可視化に時間をかける**\n",
    "   - 機械学習プロジェクトの成功の8割はデータの前処理と理解にかかっています\n",
    "   - データを様々な角度から可視化し、パターンを見つけることが重要です\n",
    "\n",
    "3. **シンプルなモデルから始める**\n",
    "   - 複雑なモデルよりも、まずはシンプルなモデル（線形回帰、決定木など）から始めましょう\n",
    "   - ベースラインとなるシンプルなモデルの性能を理解してから、徐々に複雑なモデルに挑戦するのが良いです\n",
    "\n",
    "4. **継続的に学習する姿勢を持つ**\n",
    "   - 機械学習は日々進化している分野です\n",
    "   - 最新の論文や技術に触れる機会を持ち、継続的に学び続けることが大切です\n",
    "\n",
    "5. **コミュニティに参加する**\n",
    "   - Kaggle, GitHub, Stack Overflow, Reddit（r/MachineLearning）などのコミュニティに参加する\n",
    "   - 他の学習者や専門家と交流することで、多くの学びを得ることができます\n",
    "\n",
    "### おわりに\n",
    "\n",
    "このハンズオンを通して、機械学習の基礎とニューラルネットワークの仕組みについて学びました。これはあくまで始まりに過ぎません。機械学習の世界は広大で、学ぶべきことが多くありますが、このハンズオンが皆さんの機械学習の旅の一助となれば幸いです。\n",
    "\n",
    "次のノートブック「02_deep_neural_networks_tutorial.ipynb」では、より深いニューラルネットワークについて学んでいきます。引き続き学習を楽しんでください！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25808b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
