{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704faf78",
   "metadata": {},
   "source": [
    "# 深層ニューラルネットワーク・正則化手法・高度な最適化アルゴリズム\n",
    "\n",
    "このノートブックでは、深層ニューラルネットワーク、正則化手法、高度な最適化アルゴリズムのハンズオン学習を提供します。深層ニューラルネットワークの構築、訓練、評価を行いながら、パフォーマンスを向上させるための様々なテクニックを実装していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a97b7",
   "metadata": {},
   "source": [
    "## 深層学習の基礎概念\n",
    "\n",
    "### 機械学習と深層学習\n",
    "\n",
    "**機械学習**とは、コンピュータがデータから学習し、明示的なプログラミングなしにタスクを改善する能力を指します。従来のプログラミングでは、私たちがルールとデータを提供し、コンピュータが答えを出力します。一方、機械学習では、データと答えを提供し、コンピュータ自身がルールを学習します。\n",
    "\n",
    "**深層学習**は機械学習の一種で、多層のニューラルネットワーク（深層ニューラルネットワーク）を使用して、データから表現を学習します。人間の脳の構造に着想を得たこの手法は、以下のような特徴を持ちます：\n",
    "\n",
    "- **階層的な特徴学習**: 低レベルの特徴（エッジ、色など）から徐々に高レベルの特徴（顔、物体など）を自動的に学習します\n",
    "- **表現学習**: 生データから有用な表現を自動的に発見します\n",
    "- **エンド・ツー・エンド学習**: 前処理から最終出力まで一貫した学習が可能です\n",
    "\n",
    "### なぜ深層学習なのか？\n",
    "\n",
    "深層学習は以下のような利点があります：\n",
    "\n",
    "1. **高い表現力**: 複雑なパターンや非線形関係を学習する能力があります\n",
    "2. **特徴エンジニアリングの自動化**: 手動で特徴を設計する労力を削減します\n",
    "3. **スケーラビリティ**: より多くのデータとより大きなモデルで性能が向上し続けます\n",
    "4. **転移学習**: あるタスクで学習した知識を別のタスクに転用できます\n",
    "\n",
    "### このチュートリアルの目的\n",
    "\n",
    "このチュートリアルでは、単にモデルを構築するだけでなく、深層学習の基本原理を理解し、モデル性能を向上させるための様々な技術を学びます。特に以下の点に焦点を当てています：\n",
    "\n",
    "- 深層ニューラルネットワークの基本構造と動作原理の理解\n",
    "- 過学習を防ぐための正則化手法の理解と実装\n",
    "- モデル学習の効率と効果を高める最適化アルゴリズムの比較\n",
    "- モデル評価と解釈のベストプラクティス\n",
    "\n",
    "これらの概念と技術を習得することで、実践的な深層学習モデルを開発するための強固な基盤を築くことができるでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3528671",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ab4d4",
   "metadata": {},
   "source": [
    "深層学習を実装するために必要な主要ライブラリを以下でインポートします：\n",
    "\n",
    "- **NumPy**: 数値計算のための基本ライブラリ\n",
    "- **matplotlib**: データの可視化ツール\n",
    "- **TensorFlow/Keras**: 深層学習モデルの構築と訓練のためのフレームワーク\n",
    "- **pandas**: データ操作と分析用のライブラリ\n",
    "- **seaborn**: 統計的なデータ可視化のためのライブラリ\n",
    "\n",
    "また、日本語でのグラフ表示を可能にするために `japanize-matplotlib` を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlowのバージョンを確認\n",
    "print(f\"TensorFlow バージョン: {tf.__version__}\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%pip install -U japanize-matplotlib\n",
    "\n",
    "# 日本語化ライブラリのインポート＆有効化\n",
    "import japanize_matplotlib\n",
    "japanize_matplotlib.japanize()\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# フォントファミリを Meiryo に\n",
    "plt.rcParams['font.family']       = 'Meiryo'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb680d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import japanize_matplotlib\n",
    "import sys\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Check if running on Linux\n",
    "if sys.platform.startswith('linux'):\n",
    "    # List available fonts with Japanese support\n",
    "    fonts = [f for f in fm.findSystemFonts() if 'gothic' in f.lower() or 'mincho' in f.lower() or 'meiryo' in f.lower()]\n",
    "    \n",
    "    if fonts:\n",
    "        # Use the first available Japanese font\n",
    "        plt.rcParams['font.family'] = fm.FontProperties(fname=fonts[0]).get_name()\n",
    "    else:\n",
    "        # Fallback to IPAGothic or another common Japanese font\n",
    "        plt.rcParams['font.family'] = 'IPAGothic, Noto Sans CJK JP, MS Gothic'\n",
    "else:\n",
    "    # On Windows/Mac, use platform-specific fonts\n",
    "    if sys.platform.startswith('win'):\n",
    "        plt.rcParams['font.family'] = 'MS Gothic'\n",
    "    elif sys.platform.startswith('darwin'):\n",
    "        plt.rcParams['font.family'] = 'AppleGothic'\n",
    "\n",
    "# Disable font warnings (optional)\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "japanize_matplotlib.japanize()  # Apply japanize_matplotlib settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a091b291",
   "metadata": {},
   "source": [
    "## データセットの読み込みと準備\n",
    "\n",
    "このチュートリアルでは、深層学習実験の入門としてMNISTデータセットを使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c10f28",
   "metadata": {},
   "source": [
    "### MNISTデータセットとは？\n",
    "\n",
    "**MNIST**は「Modified National Institute of Standards and Technology」の略で、手書き数字（0〜9）の画像データセットです。機械学習、特に画像認識の入門として広く使われています。\n",
    "\n",
    "**特徴：**\n",
    "- 60,000枚のトレーニング画像と10,000枚のテスト画像を含みます\n",
    "- 各画像は28×28ピクセルのグレースケール画像（0〜255の輝度値）\n",
    "- 各画像には0〜9の数字ラベルが付いています\n",
    "\n",
    "### データの前処理ステップ\n",
    "\n",
    "機械学習モデルにデータを投入する前に、いくつかの重要な前処理ステップを実行する必要があります：\n",
    "\n",
    "1. **正規化（Normalization）**: ピクセル値を0〜1の範囲に変換します。これはモデルのパフォーマンスと収束速度を向上させるために重要です。\n",
    "   - 元のピクセル値範囲：0〜255\n",
    "   - 正規化後の範囲：0〜1\n",
    "\n",
    "2. **データの平坦化（Flattening）**: 2次元の画像データ（28×28）を1次元ベクトル（784）に変換します。全結合層（Dense層）は1次元入力を想定しているため、この変換が必要となります。\n",
    "   - 元の形状：(サンプル数, 28, 28)\n",
    "   - 平坦化後：(サンプル数, 784)\n",
    "\n",
    "3. **ワンホットエンコーディング**: カテゴリラベルを二値クラス行列に変換します。\n",
    "   - 元のラベル：0〜9の単一の数値（例：「5」）\n",
    "   - 変換後：10次元のバイナリベクトル（例：[0,0,0,0,0,1,0,0,0,0]）\n",
    "\n",
    "以下のコードでは、これらの前処理ステップを具体的に実装しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012371c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNISTデータセットを読み込む\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# ピクセル値を0から1の間に正規化\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# 全結合層で使用するためにデータを平坦化\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "# クラスベクトルをバイナリクラス行列に変換（ワンホットエンコーディング）\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"訓練サンプル数: {x_train.shape[0]}\")\n",
    "print(f\"テストサンプル数: {x_test.shape[0]}\")\n",
    "print(f\"入力形状: {x_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb47d19",
   "metadata": {},
   "source": [
    "### データの可視化\n",
    "\n",
    "前処理したデータを理解するために、いくつかのサンプル画像を可視化してみましょう。これにより、モデルが実際に何を「見ている」のかをより具体的に理解できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# オリジナルデータを再度読み込み（可視化のためにのみ使用）\n",
    "(orig_x_train, orig_y_train), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# ランダムに9つの画像を選択して表示\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    # ランダムインデックスを生成\n",
    "    idx = np.random.randint(0, orig_x_train.shape[0])\n",
    "    \n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(orig_x_train[idx], cmap='gray')\n",
    "    plt.title(f\"ラベル: {orig_y_train[idx]}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"MNISTデータセットのサンプル画像\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# 前処理（平坦化と正規化）後のデータの形状を確認\n",
    "print(f\"訓練データの形状: {x_train.shape}\")\n",
    "print(f\"ワンホットエンコード後のラベル例: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d3834",
   "metadata": {},
   "source": [
    "## 深層ニューラルネットワークの構築\n",
    "\n",
    "Keras Sequential APIを使用して、基本的な深層ニューラルネットワークのアーキテクチャを定義しましょう。複数の隠れ層と適切な活性化関数を持つモデルを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025221d5",
   "metadata": {},
   "source": [
    "### ニューラルネットワークの基本構造\n",
    "\n",
    "ニューラルネットワークは、以下の主要な要素で構成されています：\n",
    "\n",
    "1. **入力層 (Input Layer)**: データがモデルに入る最初の層です。各ノードは入力特徴量の1つの値に対応します。\n",
    "   - 今回のケース：784ノード（28×28ピクセルの画像を平坦化したもの）\n",
    "\n",
    "2. **隠れ層 (Hidden Layers)**: 入力層と出力層の間にある層で、複雑なパターンを学習します。\n",
    "   - 層の数が多いほど、より複雑な関係を学習できますが、過学習のリスクも高まります。\n",
    "   - 今回は512→256→128→64ノードと徐々に小さくしていくアーキテクチャを採用しています。\n",
    "\n",
    "3. **出力層 (Output Layer)**: モデルの最終層で、タスクに応じた形式で結果を提供します。\n",
    "   - 多クラス分類（今回のケース）：クラス数に対応するノード（10個）を持ち、softmax活性化関数を使用\n",
    "\n",
    "### 活性化関数 (Activation Functions)\n",
    "\n",
    "活性化関数は、ニューロンの出力に非線形性を導入する数学的関数です：\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**: `f(x) = max(0, x)`\n",
    "   - 隠れ層でよく使用される\n",
    "   - 利点：計算が高速で、勾配消失問題を軽減\n",
    "   - 今回の隠れ層では全てこの活性化関数を採用\n",
    "\n",
    "2. **Softmax**: 複数クラスの確率分布を出力する関数\n",
    "   - 多クラス分類の出力層で使用される\n",
    "   - 全ての出力値の合計は1になる（確率として解釈可能）\n",
    "   - 今回の出力層で使用\n",
    "\n",
    "### Kerasでのモデル構築方法\n",
    "\n",
    "Kerasでは主に2つのAPIを使ってモデルを構築できます：\n",
    "\n",
    "1. **Sequential API**: 層を順番に積み重ねるシンプルなモデルに適しています。\n",
    "   - 使いやすく、初心者に推奨\n",
    "   - 線形的なネットワークアーキテクチャに最適\n",
    "\n",
    "2. **Functional API**: より複雑なモデルアーキテクチャを構築できます。\n",
    "   - マルチ入力/マルチ出力モデル\n",
    "   - 層を共有するモデル\n",
    "   - 非線形トポロジー（DAG）のモデル\n",
    "\n",
    "以下のコードでは、まずSequential APIを使って基本的なDNNを構築し、その後同じアーキテクチャをFunctional APIで実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36665cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_dnn():\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(784,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# モデルを作成\n",
    "basic_model = create_basic_dnn()\n",
    "\n",
    "# モデルをコンパイル\n",
    "basic_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# モデルの概要を表示\n",
    "basic_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d98eb3",
   "metadata": {},
   "source": [
    "### モデル概要の読み方\n",
    "\n",
    "上記の`model.summary()`の出力について解説します：\n",
    "\n",
    "- **Layer (type)**: レイヤーの名前とタイプ\n",
    "- **Output Shape**: 各レイヤーの出力テンソルの形状\n",
    "  - (None, 512) の「None」はバッチサイズを表し、可変であることを示します\n",
    "- **Param #**: レイヤーの学習可能なパラメータ数\n",
    "  - 例えば最初の層では、784（入力）× 512（出力）+ 512（バイアス）= 401,920 パラメータ\n",
    "\n",
    "### モデルのコンパイル\n",
    "\n",
    "モデルをコンパイルする際に指定する主要な要素：\n",
    "\n",
    "1. **損失関数 (Loss Function)**: モデルの予測と実際のターゲット値との差を測定\n",
    "   - 分類タスク用: `categorical_crossentropy`（多クラス）または`binary_crossentropy`（二値）\n",
    "   - 回帰タスク用: `mean_squared_error`など\n",
    "\n",
    "2. **オプティマイザ (Optimizer)**: パラメータ更新の方法を決定\n",
    "   - `adam`: 最も一般的で効果的なオプティマイザの1つ\n",
    "   - 他に`sgd`, `rmsprop`などがあります（後で詳しく学びます）\n",
    "\n",
    "3. **評価指標 (Metrics)**: モデルの性能を監視する指標\n",
    "   - `accuracy`: 分類タスクでの正確さを測定\n",
    "   - 他に`precision`, `recall`, `f1`などがあります"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f03df5",
   "metadata": {},
   "source": [
    "同じモデルアーキテクチャをより柔軟性の高いファンクショナルAPIを使って実装してみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08171023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_functional_dnn():\n",
    "    inputs = Input(shape=(784,))\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ファンクショナルAPIを使用してモデルを作成\n",
    "functional_model = create_functional_dnn()\n",
    "\n",
    "# モデルをコンパイル\n",
    "functional_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# モデルの概要を表示\n",
    "functional_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bbd7cd",
   "metadata": {},
   "source": [
    "### Sequential APIとFunctional APIの比較\n",
    "\n",
    "上記の2つの実装を比較すると：\n",
    "\n",
    "1. **Sequential API**:\n",
    "   - より直感的でシンプル\n",
    "   - コードが短く、読みやすい\n",
    "   - 単純な線形モデルに最適\n",
    "\n",
    "2. **Functional API**:\n",
    "   - より柔軟で、複雑なアーキテクチャを構築可能\n",
    "   - 各層を変数として扱うため、後で参照したり再利用したりできる\n",
    "   - マルチ入力/出力モデルやスキップ接続などの高度な機能をサポート\n",
    "\n",
    "今回は単純な順次接続モデルなので、どちらのAPIを使っても結果は同じです。より複雑なモデル（ResNet, U-Net, SiameseNetなど）を実装する場合は、Functional APIが必要になることが多いでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88ac19",
   "metadata": {},
   "source": [
    "## 正則化テクニックの適用\n",
    "\n",
    "過学習を防ぐためのさまざまな正則化テクニックを探ってみましょう：\n",
    "\n",
    "1. L1正則化（Lasso）- 重みの絶対値にペナルティを課すことでスパース性を促進します\n",
    "2. L2正則化（Ridge）- 重みの二乗和を加えることで大きな重みにペナルティを課します\n",
    "3. ドロップアウト - トレーニング中にランダムに一部の入力ユニットを0に設定します\n",
    "\n",
    "これらの正則化テクニックを使用したモデルを作成し、それらのパフォーマンスを比較しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c0c31",
   "metadata": {},
   "source": [
    "### 過学習（オーバーフィッティング）の問題\n",
    "\n",
    "深層学習モデルを訓練する際に直面する主要な課題の1つが**過学習**です。これは、モデルが訓練データに対して過度に最適化され、新しいデータに対する汎化性能が低下する現象です。\n",
    "\n",
    "**過学習の兆候**：\n",
    "- 訓練データでの性能は非常に良いが、検証/テストデータでの性能が悪い\n",
    "- 訓練誤差と検証誤差の間に大きな差がある\n",
    "- 複雑なモデル（多くのパラメータを持つモデル）ほど過学習しやすい\n",
    "\n",
    "### 正則化とは？\n",
    "\n",
    "**正則化**とは、モデルの複雑さに制約を加えることで過学習を防ぎ、汎化性能を向上させる技術です。モデルを「よりシンプルに」保つことを促進します。\n",
    "\n",
    "### L1正則化（Lasso正則化）\n",
    "\n",
    "**仕組み**：重みの絶対値の和にペナルティを課します。\n",
    "\n",
    "**数学的表現**：損失関数 + λ * Σ|w|（λは正則化の強度を制御するハイパーパラメータ）\n",
    "\n",
    "**特徴**：\n",
    "- **スパース性の促進**: 多くの重みを正確に0にする傾向があります\n",
    "- **特徴選択**: 重要でない特徴を自動的に除外します\n",
    "- **解釈可能性**: スパースなモデルはより解釈しやすい\n",
    "\n",
    "**適用例**：特徴量が多く、一部が冗長または不要な場合に有効\n",
    "\n",
    "### L2正則化（Ridge正則化）\n",
    "\n",
    "**仕組み**：重みの二乗和にペナルティを課します。\n",
    "\n",
    "**数学的表現**：損失関数 + λ * Σw²\n",
    "\n",
    "**特徴**：\n",
    "- **重みの縮小**: すべての重みを小さくしますが、完全に0にはしません\n",
    "- **安定性**: 相関のある特徴に対して安定した解を提供します\n",
    "- **より滑らかな解**: 極端な重みを避けます\n",
    "\n",
    "**適用例**：マルチコ線形性（特徴間の高い相関）がある場合に効果的\n",
    "\n",
    "### ドロップアウト\n",
    "\n",
    "**仕組み**：トレーニング中にランダムにニューロンを「ドロップアウト」（一時的に無効化）します。\n",
    "\n",
    "**具体的な動作**：\n",
    "- 各訓練バッチで、指定された割合（例：30%）のニューロンがランダムに選ばれ、その出力が0に設定されます\n",
    "- 推論時には、全てのニューロンが活性化され、訓練時との出力スケールの差を調整するために重みが調整されます\n",
    "\n",
    "**特徴**：\n",
    "- **アンサンブル効果**: 複数のサブネットワークを同時に訓練するような効果があります\n",
    "- **共適応の防止**: ニューロンが特定のニューロンに過度に依存するのを防ぎます\n",
    "- **ノイズへの頑健性**: ランダムノイズを加えることで、より堅牢なモデルになります\n",
    "\n",
    "**適用例**：大規模なニューラルネットワークで特に効果的\n",
    "\n",
    "### 複数の正則化手法の組み合わせ\n",
    "\n",
    "実際には、複数の正則化テクニックを組み合わせることで、より効果的に過学習を防ぐことができます。例えば、L2正則化とドロップアウトを組み合わせると：\n",
    "\n",
    "- L2正則化は重みの大きさを制限\n",
    "- ドロップアウトはニューロン間の依存関係を減少\n",
    "\n",
    "これらは互いに補完し合い、より堅牢なモデルを作成するのに役立ちます。以下のコードでは、これらの正則化テクニックを実装したモデルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1正則化を使用したモデル\n",
    "def create_l1_model():\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=l1(0.001)),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l1(0.001)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.001)),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(0.001)),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# L2正則化を使用したモデル\n",
    "def create_l2_model():\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=l2(0.001)),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ドロップアウトを使用したモデル\n",
    "def create_dropout_model():\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(784,)),\n",
    "        Dropout(0.3),  # 30%のドロップアウト率\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# L2正則化とドロップアウトの両方を使用したモデル\n",
    "def create_combined_reg_model():\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# モデルを作成してコンパイルする\n",
    "l1_model = create_l1_model()\n",
    "l1_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "l2_model = create_l2_model()\n",
    "l2_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "dropout_model = create_dropout_model()\n",
    "dropout_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "combined_model = create_combined_reg_model()\n",
    "combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# モデルの概要を表示\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618a8b8",
   "metadata": {},
   "source": [
    "### 正則化パラメータの選択\n",
    "\n",
    "正則化の効果は、正則化パラメータ（λ値やドロップアウト率）に大きく依存します：\n",
    "\n",
    "- **λが小さすぎると**: 正則化の効果が弱く、過学習を防げない可能性があります\n",
    "- **λが大きすぎると**: モデルが単純化されすぎて、訓練データにも適合できなくなる「過少適合（アンダーフィッティング）」が発生する可能性があります\n",
    "- **ドロップアウト率が低すぎると**: 効果が薄くなります\n",
    "- **ドロップアウト率が高すぎると**: 情報の流れが妨げられ、モデルの学習能力が低下します\n",
    "\n",
    "最適な正則化パラメータは、交差検証（Cross-validation）などを使って実験的に決定するのが一般的です。問題やデータセットによって最適値は異なります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8645ed",
   "metadata": {},
   "source": [
    "## 高度な最適化アルゴリズムの実装\n",
    "\n",
    "さまざまな最適化アルゴリズムとテクニックを探っていきましょう：\n",
    "1. 標準的な確率的勾配降下法（Standard SGD）\n",
    "2. モーメンタム付きSGD（SGD with momentum）\n",
    "3. RMSprop\n",
    "4. Adam\n",
    "5. 学習率スケジューリング（Learning rate scheduling）\n",
    "\n",
    "これらの異なる最適化手法を用いてモデルを訓練し、収束性能とパフォーマンスを比較してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ccc4a",
   "metadata": {},
   "source": [
    "### 最適化アルゴリズムの基本概念\n",
    "\n",
    "**最適化アルゴリズム**は、モデルの損失関数を最小化するために、モデルのパラメータ（重みとバイアス）をどのように更新するかを決定するアルゴリズムです。深層学習における最適化は、複雑で非凸な高次元空間での最小値探索という難しい問題です。\n",
    "\n",
    "### 勾配降下法の基本\n",
    "\n",
    "すべての最適化アルゴリズムは、**勾配降下法**の基本概念に基づいています。勾配降下法では、現在のパラメータ位置での損失関数の勾配（傾き）を計算し、その勾配の方向に逆らってパラメータを更新します。\n",
    "\n",
    "基本的な更新式：  \n",
    "θ = θ - η∇J(θ)  \n",
    "ここで、  \n",
    "- θ：モデルパラメータ  \n",
    "- η：学習率（ステップサイズ）  \n",
    "- ∇J(θ)：θに関する損失関数の勾配  \n",
    "\n",
    "### 確率的勾配降下法（SGD）\n",
    "\n",
    "**確率的勾配降下法（SGD）** は、各パラメータ更新にトレーニングデータの小さなランダムサブセット（ミニバッチ）を使用します。\n",
    "\n",
    "**特徴**：\n",
    "- **単純**: 実装が簡単で計算コストが低い\n",
    "- **ノイズ**: バッチが異なるため更新が揺らぎ、局所的最小値から抜け出す可能性がある\n",
    "- **課題**: 収束が遅く、最適なパスを見つけるのに苦労することがある（「ジグザグ」パターンで進む）\n",
    "\n",
    "### モーメンタム付きSGD\n",
    "\n",
    "**モーメンタム**は、以前の更新方向の「慣性」を加えることで通常のSGDを拡張します。物理学における慣性の概念に着想を得ています。\n",
    "\n",
    "更新式：  \n",
    "v = γv - η∇J(θ)  \n",
    "θ = θ + v  \n",
    "ここで、v はベロシティ（速度）ベクトル、γ はモーメンタム係数（通常0.9前後）\n",
    "\n",
    "**利点**：\n",
    "- **収束の加速**: 勾配が同じ方向を指す場合、更新幅が大きくなる\n",
    "- **振動の軽減**: 勾配が方向を変える場合、ジグザグパターンが減少\n",
    "- **局所最小値からの脱出**: 慣性により小さな局所的最小値を超えられる可能性\n",
    "\n",
    "### RMSprop\n",
    "\n",
    "**RMSprop** (Root Mean Square Propagation) は、パラメータごとに異なる学習率を適応的に調整するアルゴリズムです。\n",
    "\n",
    "更新式：  \n",
    "E[g²]t = 0.9E[g²]t-1 + 0.1g²t  \n",
    "θ = θ - η / √(E[g²]t + ε) * gt  \n",
    "ここで、E[g²]t は過去の勾配二乗の移動平均、ε は小さな値（ゼロ除算防止）\n",
    "\n",
    "**利点**：\n",
    "- **適応的学習率**: 勾配が大きいパラメータは小さく更新し、小さいパラメータは大きく更新する\n",
    "- **対称性の解消**: 特徴のスケールが異なる場合に有効\n",
    "- **振動の抑制**: 急な次元での更新を抑制する\n",
    "\n",
    "### Adam（Adaptive Moment Estimation）\n",
    "\n",
    "**Adam**は、モーメンタムとRMSpropの利点を組み合わせた最適化手法です。2つのモーメント（一次モーメント：平均、二次モーメント：分散）を計算します。\n",
    "\n",
    "簡略化した更新式：  \n",
    "m = β1m + (1-β1)g  # 勾配の移動平均（一次モーメント）  \n",
    "v = β2v + (1-β2)g²  # 勾配二乗の移動平均（二次モーメント）  \n",
    "θ = θ - η * m / (√v + ε)  # バイアス補正後の更新  \n",
    "\n",
    "**利点**：\n",
    "- **収束性**: 通常、他の最適化手法より速く収束する\n",
    "- **メモリ効率**: 追加のメモリ消費が少ない\n",
    "- **ハイパーパラメータ頑健性**: デフォルトのハイパーパラメータが多くの場合でうまく機能する\n",
    "- **現在の事実上の標準**: 多くの深層学習タスクでデフォルトの選択肢となっている\n",
    "\n",
    "### 学習率スケジューリング\n",
    "\n",
    "**学習率スケジューリング**は、トレーニング中に学習率を動的に調整する技術です。\n",
    "\n",
    "**主な戦略**：\n",
    "\n",
    "1. **ステップ減衰**: 特定のエポック数後に学習率を段階的に減少させる（このノートブックで使用）\n",
    "2. **指数減衰**: 各エポックで学習率を指数関数的に減少させる（例：lr = lr0 * e^(-kt)）\n",
    "3. **1/t減衰**: 学習率をエポック数に反比例して減少させる（例：lr = lr0 / (1 + kt)）\n",
    "4. **コサイン減衰**: 学習率をコサイン関数に従って減少させる\n",
    "5. **ウォームアップ**: 小さな学習率から始め、徐々に増加させた後、再び減少させる\n",
    "\n",
    "**利点**：\n",
    "- **初期の高速収束**: 最初は大きな学習率で素早く収束\n",
    "- **後期の微調整**: 後半は小さな学習率で細かく調整し、最適解に近づける\n",
    "- **振動の軽減**: トレーニング後期の振動を抑制する\n",
    "\n",
    "### 最適化アルゴリズムの選択ガイド\n",
    "\n",
    "- **初めての場合**: Adamを選択（汎用性が高く、多くの場合でうまく機能する）\n",
    "- **計算リソースが限られている**: SGDまたはモーメンタム付きSGDを検討\n",
    "- **過学習が懸念される**: 学習率スケジューリングを追加\n",
    "- **最良の性能を求める**: 複数の最適化手法を試し、検証セットで比較\n",
    "\n",
    "以下のコードでは、これらの異なる最適化アルゴリズムを実装しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04eb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の比較用にシンプルなモデルを作成する関数\n",
    "def create_simple_model():\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(784,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 学習率スケジューラを定義\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    if epoch < 5:\n",
    "        return initial_lr\n",
    "    elif epoch < 10:\n",
    "        return initial_lr * 0.5\n",
    "    else:\n",
    "        return initial_lr * 0.1\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# 異なるオプティマイザでモデルを作成\n",
    "# 標準SGD\n",
    "sgd_model = create_simple_model()\n",
    "sgd_model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# モーメンタム付きSGD\n",
    "sgd_momentum_model = create_simple_model()\n",
    "sgd_momentum_model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# RMSprop\n",
    "rmsprop_model = create_simple_model()\n",
    "rmsprop_model.compile(\n",
    "    optimizer=RMSprop(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Adam\n",
    "adam_model = create_simple_model()\n",
    "adam_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 学習率スケジューリング付きAdam\n",
    "adam_lr_model = create_simple_model()\n",
    "adam_lr_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20743f1",
   "metadata": {},
   "source": [
    "### 最適化アルゴリズムの実験と可視化\n",
    "\n",
    "以下では、学習中の振る舞いを理解するために、一部のモデルをトレーニングし、それらのパフォーマンスを比較します。完全な比較のためには、すべてのモデルをトレーニングしてみることもできますが、時間を節約するために、ここではAdamと学習率スケジューリング付きAdamのみを比較します。\n",
    "\n",
    "**データ準備のポイント**:\n",
    "- トレーニング高速化のために、元の訓練データセットの一部（10,000サンプル）のみを使用します\n",
    "- トレーニング中のモデル性能をリアルタイムで監視するために、別の検証セット（2,000サンプル）を用意します\n",
    "- バッチサイズ128でエポック数10でトレーニングします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迅速なデモンストレーションのために小さなサブセットを使用\n",
    "subset_size = 10000\n",
    "x_train_subset = x_train[:subset_size]\n",
    "y_train_subset = y_train[:subset_size]\n",
    "x_val = x_train[subset_size:subset_size+2000]\n",
    "y_val = y_train[subset_size:subset_size+2000]\n",
    "\n",
    "# Adamモデルをトレーニング\n",
    "adam_history = adam_model.fit(\n",
    "    x_train_subset, y_train_subset,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 学習率スケジューリング付きAdamモデルをトレーニング\n",
    "adam_lr_history = adam_lr_model.fit(\n",
    "    x_train_subset, y_train_subset,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[lr_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c828d67",
   "metadata": {},
   "source": [
    "## モデルのパフォーマンス評価\n",
    "\n",
    "モデルを評価し、学習曲線を通して訓練プロセスを可視化してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a67a1",
   "metadata": {},
   "source": [
    "### モデル評価の重要性\n",
    "\n",
    "モデル評価は機械学習プロセスの重要な部分です。評価することで以下のような疑問に答えられます：\n",
    "\n",
    "- モデルはどれくらい正確に予測できるか？\n",
    "- モデルは過学習（オーバーフィッティング）しているか？\n",
    "- どのモデル設計や最適化手法が最も効果的か？\n",
    "- モデルはさらなる改善が必要か？\n",
    "\n",
    "### 学習曲線の解釈\n",
    "\n",
    "**学習曲線（Learning Curves）** は、訓練の進行に伴うモデル性能の変化を示すグラフです。以下のポイントに注目することで多くの洞察が得られます：\n",
    "\n",
    "1. **訓練精度と検証精度の比較**:\n",
    "   - 訓練精度 > 検証精度（大きな差）→ 過学習の可能性大\n",
    "   - 訓練精度 ≈ 検証精度 → モデルは適切に汎化している\n",
    "   - 両方の精度が低い → 過少適合（アンダーフィッティング）\n",
    "\n",
    "2. **学習曲線の形状**:\n",
    "   - 平坦化しているか？ → モデルが学習の上限に達している\n",
    "   - 依然として上昇/下降しているか？ → さらに訓練することで改善の可能性あり\n",
    "   - 不安定か？ → 学習率が高すぎるか、バッチサイズが小さすぎる可能性\n",
    "\n",
    "3. **検証損失の動向**:\n",
    "   - 増加し始める → 過学習の開始\n",
    "   - 常に減少 → より良い汎化性能\n",
    "\n",
    "### 学習率スケジューリングの効果\n",
    "\n",
    "学習率スケジューリングがモデルのパフォーマンスに与える影響を観察することで、次のような点が理解できます：\n",
    "\n",
    "- 学習率が低くなると、損失関数の変化が穏やかになる\n",
    "- 訓練後半での性能向上や安定化\n",
    "- 学習率の急激な変化は学習曲線の急激な変化として現れることがある\n",
    "\n",
    "以下のコードでは、異なる最適化戦略（標準Adamと学習率スケジューリング付きAdam）の学習曲線を比較可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線をプロットする関数\n",
    "def plot_learning_curves(histories, labels, metric='accuracy'):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 訓練メトリクスのプロット\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, history in enumerate(histories):\n",
    "        plt.plot(history.history[metric], label=f'{labels[i]} 訓練')\n",
    "    plt.title(f'訓練 {metric.capitalize()}')\n",
    "    plt.xlabel('エポック')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend()\n",
    "    \n",
    "    # 検証メトリクスのプロット\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, history in enumerate(histories):\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'{labels[i]} 検証')\n",
    "    plt.title(f'検証 {metric.capitalize()}')\n",
    "    plt.xlabel('エポック')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 訓練と検証の損失をプロット\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 訓練損失のプロット\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, history in enumerate(histories):\n",
    "        plt.plot(history.history['loss'], label=f'{labels[i]} 訓練')\n",
    "    plt.title('訓練損失')\n",
    "    plt.xlabel('エポック')\n",
    "    plt.ylabel('損失')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 検証損失のプロット\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, history in enumerate(histories):\n",
    "        plt.plot(history.history['val_loss'], label=f'{labels[i]} 検証')\n",
    "    plt.title('検証損失')\n",
    "    plt.xlabel('エポック')\n",
    "    plt.ylabel('損失')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# AdamとLRスケジューリング付きAdamの学習曲線をプロット\n",
    "plot_learning_curves(\n",
    "    [adam_history, adam_lr_history],\n",
    "    ['Adam', 'LRスケジューリング付きAdam']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1447d2e",
   "metadata": {},
   "source": [
    "正則化モデルの1つをトレーニングし、テストセットでその性能を評価してみましょう："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e0f25",
   "metadata": {},
   "source": [
    "### テストセット評価の意義\n",
    "\n",
    "モデル開発の最終段階では、訓練や検証に使用していない独立したデータセット（**テストセット**）でモデルを評価することが重要です。これにより、モデルの実際の汎化性能（未知のデータに対する性能）を評価できます。\n",
    "\n",
    "テストセットは以下の特徴を持つべきです：\n",
    "\n",
    "- 訓練・検証データと同じ分布から抽出されたもの\n",
    "- モデル開発中には一切使用していないもの（モデルにとって完全に「未知」であること）\n",
    "- 将来のデータを適切に代表するもの\n",
    "\n",
    "### 予測結果の視覚化\n",
    "\n",
    "モデルの予測を視覚化することで、以下のような洞察が得られます：\n",
    "\n",
    "- どのような入力で誤分類が発生しやすいか\n",
    "- 誤分類のパターンや傾向があるか（例：特定の数字が別の数字と混同されやすいなど）\n",
    "- モデルの信頼度（確率出力）はどの程度か\n",
    "\n",
    "以下のコードでは、L2正則化とドロップアウトを組み合わせたモデルを訓練し、テストセットでの性能を評価します。さらに、いくつかのテストサンプルとその予測結果を可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 組み合わせ正則化モデルをトレーニング\n",
    "combined_history = combined_model.fit(\n",
    "    x_train_subset, y_train_subset,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# テストセットで評価\n",
    "test_scores = combined_model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f\"テスト精度: {test_scores[1]:.4f}\")\n",
    "\n",
    "# テストセットで予測を行う\n",
    "y_pred = combined_model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# いくつかのサンプル画像と予測結果を表示\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"正解: {y_true_classes[i]}, 予測: {y_pred_classes[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe63abd4",
   "metadata": {},
   "source": [
    "### 混同行列（Confusion Matrix）で詳細な分析\n",
    "\n",
    "混同行列は、分類モデルの性能をより詳細に分析するのに役立つツールです。各クラスについて予測されたラベルと実際のラベルの関係を示します。\n",
    "\n",
    "以下のコードでは、混同行列を作成し可視化します。これにより、どの数字が最も誤分類されやすいか、そしてどのような混同パターンがあるかを識別できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e44ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 混同行列を計算\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# 混同行列を可視化\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('予測ラベル')\n",
    "plt.ylabel('真のラベル')\n",
    "plt.title('混同行列')\n",
    "plt.show()\n",
    "\n",
    "# 詳細な分類レポートを表示\n",
    "print(\"\\n分類レポート:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=[str(i) for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559284c4",
   "metadata": {},
   "source": [
    "## まとめと重要なポイント\n",
    "\n",
    "このノートブックでは、以下の内容について探求しました：\n",
    "\n",
    "1. Sequential APIとFunctional APIの両方を使用した深層ニューラルネットワークの構築\n",
    "2. さまざまな正則化テクニックの実装：\n",
    "   - L1正則化（Lasso）\n",
    "   - L2正則化（Ridge）\n",
    "   - ドロップアウト\n",
    "   - 複数の正則化手法の組み合わせ\n",
    "   \n",
    "3. 異なる最適化アルゴリズムの実験：\n",
    "   - SGD（確率的勾配降下法）\n",
    "   - モーメンタム付きSGD\n",
    "   - RMSprop\n",
    "   - Adam\n",
    "   - 学習率スケジューリング\n",
    "   \n",
    "4. モデルのパフォーマンス評価と学習プロセスの可視化\n",
    "\n",
    "重要なポイント：\n",
    "- より深いネットワークはより複雑なパターンを学習できますが、過学習しやすくなります\n",
    "- 正則化テクニックは過学習を防ぎ、汎化性能を向上させるのに役立ちます\n",
    "- Adamなどの高度な最適化アルゴリズムは、基本的なSGDよりも通常速く収束します\n",
    "- 学習率スケジューリングは最適化プロセスの微調整に役立ちます\n",
    "- 複数の正則化テクニックを組み合わせると、単一のアプローチを使用するよりも良い結果が得られることが多いです"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e6568",
   "metadata": {},
   "source": [
    "### 次のステップ\n",
    "\n",
    "深層ニューラルネットワークの基本を理解したところで、以下のような発展的なトピックに取り組むことができます：\n",
    "\n",
    "1. **畳み込みニューラルネットワーク（CNN）**: 画像処理タスクのための特殊なアーキテクチャ\n",
    "2. **リカレントニューラルネットワーク（RNN）**: シーケンシャルデータ処理のためのアーキテクチャ\n",
    "3. **転移学習**: 事前訓練されたモデルを使用して、少ないデータでも高性能なモデルを構築\n",
    "4. **ハイパーパラメータチューニング**: グリッドサーチやランダムサーチを使用したモデルの最適化\n",
    "5. **モデルの解釈可能性**: モデルの予測がなぜそのような結果になるかを理解するための技術\n",
    "\n",
    "ニューラルネットワークの学習と実装を続けながら、実際の問題に適用することで理解を深めていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04161d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
