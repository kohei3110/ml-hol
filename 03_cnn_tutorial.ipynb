{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1149b3",
   "metadata": {},
   "source": [
    "# 畳み込みニューラルネットワーク（CNN）チュートリアル\n",
    "\n",
    "このノートブックでは、畳み込みニューラルネットワーク（CNN）について詳細なハンズオン形式で説明します。CNNは画像のようなグリッド状のデータを処理するために特別に設計された強力なディープラーニングアーキテクチャです。標準的な画像デセットを使用して、CNNモデルの構築、トレーニング、評価を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441b409",
   "metadata": {},
   "source": [
    "## CNNの基礎: なぜ画像処理に最適なのか？\n",
    "\n",
    "### 従来のニューラルネットワークの限界\n",
    "\n",
    "通常のニューラルネットワーク（全結合型）では、入力画像のピクセルをすべて個別の入力として扱います。例えば32x32のカラー画像（3チャネル）では、入力ニューロン数が32x32x3=3072個にもなります。これには以下の問題があります：\n",
    "\n",
    "1. **パラメータ数の爆発**: 各ニューロン間の接続が多すぎて計算効率が悪い\n",
    "2. **空間構造の無視**: 画像の2次元的な空間構造を考慮していない\n",
    "3. **位置変化に弱い**: 同じ物体が画像内で少し移動しただけで全く異なる入力として認識される\n",
    "\n",
    "### CNNの優位性\n",
    "\n",
    "CNNはこれらの問題を以下のような特徴で解決します：\n",
    "\n",
    "1. **局所的受容野（Local Receptive Field）**: 各ニューロンは画像の小さな領域だけを見る\n",
    "2. **パラメータ共有（Parameter Sharing）**: 同じフィルターを画像全体に適用するので、パラメータ数を大幅に削減\n",
    "3. **平行移動不変性（Translation Invariance）**: 同じ特徴が画像内のどこにあっても検出できる\n",
    "\n",
    "これにより、CNNは画像から階層的に特徴を抽出できるようになり、画像認識タスクで非常に高い性能を発揮します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87e753",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c26406",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install japanize-matplotlib\n",
    "\n",
    "# TensorFlow、Keras、NumPy、Matplotlibなど必要なライブラリをインポート\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 再現性のためにランダムシードを設定\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ライブラリのバージョンを表示\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.keras.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c7017",
   "metadata": {},
   "source": [
    "## データセットの読み込みと前処理\n",
    "\n",
    "このチュートリアルでは、CIFAR-10データセットを使用します。このデータセットは10クラスの32x32カラー画像60,000枚から構成されており、50,000枚の訓練画像と10,000枚のテスト画像に分割されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10データセットを読み込む\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# データセットの形状を表示\n",
    "print(f\"訓練データの形状: {x_train.shape}\")\n",
    "print(f\"訓練ラベルの形状: {y_train.shape}\")\n",
    "print(f\"テストデータの形状: {x_test.shape}\")\n",
    "print(f\"テストラベルの形状: {y_test.shape}\")\n",
    "\n",
    "# CIFAR-10のクラス名を定義\n",
    "class_names = ['飛行機', '自動車', '鳥', '猫', '鹿', \n",
    "               '犬', 'カエル', '馬', '船', 'トラック']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b53008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットからサンプル画像を可視化\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.xlabel(class_names[y_train[i][0]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170c8a3",
   "metadata": {},
   "source": [
    "## データ前処理の重要性\n",
    "\n",
    "機械学習、特にディープラーニングにおいてデータ前処理は非常に重要な要素です。適切な前処理を行うことで、モデルの学習効率や精度が大幅に向上します。\n",
    "\n",
    "### ピクセル値の正規化が必要な理由\n",
    "\n",
    "1. **勾配降下法の安定化**: 入力値の範囲が大きいと勾配爆発や勾配消失の問題が発生しやすくなります。正規化により、これらの問題を軽減できます。\n",
    "\n",
    "2. **収束の高速化**: すべての特徴が同じスケールにあると、最適化アルゴリズムが効率的に機能し、より速く収束します。\n",
    "\n",
    "3. **数値的安定性**: コンピュータの浮動小数点計算は小さな値の範囲で最も精度が高いため、0から1の範囲に正規化すると計算精度が向上します。\n",
    "\n",
    "### One-Hotエンコーディングの役割\n",
    "\n",
    "ラベルデータに対しては、One-Hotエンコーディングを適用します：\n",
    "\n",
    "1. **カテゴリ間の等距離性**: 数値ラベル（0,1,2など）では、ラベル間に順序関係が暗示されますが、実際のクラスカテゴリには順序関係がありません。One-Hotエンコーディングにより、すべてのクラスが等距離になります。\n",
    "\n",
    "2. **多クラス分類との相性**: ソフトマックス活性化関数を持つ出力層とカテゴリカルクロスエントロピー損失関数は、One-Hotエンコードされたラベルとの相性が良く、確率分布として解釈できます。\n",
    "\n",
    "3. **勾配計算の効率化**: One-Hotエンコードされたベクトルは、損失関数の勾配計算を簡素化し、効率的にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ピクセル値を0から1の間に正規化\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# クラスラベルをone-hotエンコーディングに変換\n",
    "y_train_one_hot = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_one_hot = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"元のラベルの形状:\", y_train.shape)\n",
    "print(\"one-hotエンコーディング後の形状:\", y_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6b42e",
   "metadata": {},
   "source": [
    "## シンプルなCNNモデルの構築\n",
    "\n",
    "ここでCNNアーキテクチャを定義します。CNNは一般的に畳み込み層、プーリング層、全結合層から構成されており、各層は特定の役割を担っています：\n",
    "\n",
    "1. **畳み込み層（Convolutional layers）**：入力画像から特徴を抽出\n",
    "2. **プーリング層（Pooling layers）**：特徴マップの空間的次元を削減\n",
    "3. **全結合層（Dense layers）**：抽出された特徴に基づいて分類を実行\n",
    "\n",
    "このチュートリアルでは、複数の畳み込み層とプーリング層、その後に全結合層が続くシンプルなアーキテクチャを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNモデルの構築\n",
    "model = Sequential([\n",
    "    # 第1畳み込みブロック\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # 第2畳み込みブロック\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # 平坦化と全結合層\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# モデルの要約を表示\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3e319",
   "metadata": {},
   "source": [
    "## CNNの各レイヤーの働きを詳細に理解する\n",
    "\n",
    "### 1. 畳み込み層 (Conv2D)\n",
    "\n",
    "```python\n",
    "Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3))\n",
    "```\n",
    "\n",
    "- **フィルター数 (32)**: 32個の異なるパターンを検出するフィルターを学習します。初期層では単純なエッジや色の変化を、深い層ではより複雑な形状やパターンを検出します。\n",
    "\n",
    "- **カーネルサイズ (3, 3)**: 3×3ピクセルの小さな領域を一度に見る「窓」のサイズです。小さいカーネルほど細かい特徴を捉えられますが、計算コストが高くなります。\n",
    "\n",
    "- **パディング ('same')**: 入力画像の周りに0を追加して、出力の空間的次元が入力と同じになるようにします。'same'を使うと、エッジ情報が保持され、特徴マップのサイズが維持されます。\n",
    "\n",
    "- **活性化関数 ('relu')**: Rectified Linear Unit。負の値を0にし、正の値はそのまま通します。非線形性を導入して複雑なパターンを学習可能にし、勾配消失問題も軽減します。\n",
    "\n",
    "- **畳み込み処理の流れ**: フィルターが画像全体を少しずつスライドしながら走査し、入力の局所領域との内積を計算します。この結果が特徴マップとなります。\n",
    "\n",
    "### 2. プーリング層 (MaxPooling2D)\n",
    "\n",
    "```python\n",
    "MaxPooling2D(pool_size=(2, 2))\n",
    "```\n",
    "\n",
    "- **目的**: 特徴マップの空間的次元を縮小し、計算量を削減しながら重要な情報を保持します。\n",
    "\n",
    "- **動作原理**: 2×2の領域内の最大値のみを取り、出力サイズを半分に縮小します。これにより：\n",
    "  1. パラメータ数と計算量を減らす\n",
    "  2. 特徴の位置に対する小さな変動に対して頑健になる（位置不変性）\n",
    "  3. 過学習を抑制する\n",
    "\n",
    "### 3. ドロップアウト層 (Dropout)\n",
    "\n",
    "```python\n",
    "Dropout(0.25)\n",
    "```\n",
    "\n",
    "- **動作原理**: 訓練時にランダムに選ばれたニューロンを一時的に「無効化」します。この例では、25%のニューロンが各訓練ステップでランダムに無視されます。\n",
    "\n",
    "- **目的**: モデルが特定の特徴に過度に依存することを防ぎ、より汎用的な特徴を学習することを促します。これは正則化テクニックの一つであり、過学習を防ぐのに効果的です。\n",
    "\n",
    "### 4. 平坦化層 (Flatten)\n",
    "\n",
    "```python\n",
    "Flatten()\n",
    "```\n",
    "\n",
    "- **目的**: 3D特徴マップ（高さ×幅×チャネル）を1Dベクトルに変換し、全結合層への入力として使えるようにします。\n",
    "- **空間情報**: この変換により空間的な配置情報は失われますが、それまでの畳み込み層とプーリング層で抽出された特徴は保持されます。\n",
    "\n",
    "### 5. 全結合層 (Dense)\n",
    "\n",
    "```python\n",
    "Dense(512, activation='relu')\n",
    "```\n",
    "\n",
    "- **ニューロン数 (512)**: 512個のニューロンを持ち、前層からの全ての入力と接続します。\n",
    "- **目的**: 畳み込み層で抽出された特徴を組み合わせ、高レベルの抽象化を行います。\n",
    "\n",
    "### 6. 出力層\n",
    "\n",
    "```python\n",
    "Dense(10, activation='softmax')\n",
    "```\n",
    "\n",
    "- **ニューロン数 (10)**: CIFAR-10の10クラスに対応する10個のニューロン。\n",
    "- **ソフトマックス活性化**: 出力を確率分布に変換し、各クラスに属する確率を表します。すべての出力の合計は1になります。\n",
    "\n",
    "### CNNの情報処理の流れ\n",
    "\n",
    "1. 画像が入力されると、最初の畳み込み層は基本的な特徴（エッジ、テクスチャ、色の変化など）を検出\n",
    "2. プーリング層で特徴マップの次元を縮小し、重要な特徴を保持\n",
    "3. 次の畳み込み層でより高度な特徴（形状、パターンなど）を検出\n",
    "4. さらにプーリングで次元を縮小\n",
    "5. 平坦化で3D特徴マップを1Dベクトルに変換\n",
    "6. 全結合層で抽出された特徴を組み合わせて高レベルの抽象化を行う\n",
    "7. 最終的な出力層で10クラスのいずれかに分類\n",
    "\n",
    "このように、CNNは画像から徐々に複雑さを増す特徴抽出の階層構造を持っており、初期層から後期層に向かうにつれて抽象度の高い特徴を捉えていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1039c1ad",
   "metadata": {},
   "source": [
    "## CNNアーキテクチャの理解\n",
    "\n",
    "CNNモデルのアーキテクチャを詳しく見ていきましょう：\n",
    "\n",
    "1. **入力層**：32x32のRGB画像（3チャネル）を受け取ります\n",
    "\n",
    "2. **第1畳み込みブロック**：\n",
    "   - 3x3サイズの32フィルターを持つ2つの畳み込み層（ReLU活性化関数使用）\n",
    "   - 空間的次元を半分に減らすMaxPooling層\n",
    "   - 過学習を防ぐためのDropout層（25%）\n",
    "\n",
    "3. **第2畳み込みブロック**：\n",
    "   - 3x3サイズの64フィルターを持つ2つの畳み込み層（ReLU活性化関数使用）\n",
    "   - さらに空間的次元を減らすMaxPooling層\n",
    "   - Dropout層（25%）\n",
    "\n",
    "4. **全結合層**：\n",
    "   - 3次元特徴マップを1次元特徴ベクトルに変換するFlatten層\n",
    "   - 512ニューロンとReLU活性化関数を持つDense層\n",
    "   - Dropout層（50%）\n",
    "   - 10ニューロン（クラスごとに1つ）とソフトマックス活性化関数を持つ出力層\n",
    "\n",
    "このアーキテクチャは一般的なCNN設計パターンに従っています：\n",
    "- 徐々に複雑な特徴を抽出するための複数の畳み込み層\n",
    "- 次元と計算の複雑さを削減するためのプーリング層\n",
    "- 過学習を防ぐための正則化としてのDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59de620",
   "metadata": {},
   "source": [
    "## CNNモデルの学習\n",
    "\n",
    "ここで、訓練データを使用してCNNモデルをコンパイルし学習させます。次のものを使用します：\n",
    "- Adamオプティマイザ：適応的な学習率最適化アルゴリズム\n",
    "- カテゴリカルクロスエントロピー損失：多クラス分類に適した損失関数\n",
    "- 精度を評価指標として使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618c8f8",
   "metadata": {},
   "source": [
    "### データ拡張の重要性\n",
    "\n",
    "**データ拡張（Data Augmentation）** は、モデルの汎化性能を向上させるための重要なテクニックです。限られたトレーニングデータからモデルがより多様なパターンを学習できるようにします。\n",
    "\n",
    "#### データ拡張の利点\n",
    "\n",
    "1. **過学習の軽減**: 同じ画像を様々な変形で見せることで、モデルはデータの本質的な特徴に集中し、訓練データの細かな特徴に過度に適応することを防ぎます。\n",
    "\n",
    "2. **データセットサイズの実質的増加**: 物理的なデータ量は変わりませんが、モデルにとっては新しいデータを見ているのと同じ効果があります。\n",
    "\n",
    "3. **位置不変性の強化**: 回転、移動、反転などの変換に対して頑健なモデルが学習されます。\n",
    "\n",
    "#### 今回使用するデータ拡張\n",
    "\n",
    "- **回転 (`rotation_range=15`)**: 画像を±15度の範囲でランダムに回転\n",
    "- **水平移動 (`width_shift_range=0.1`)**: 画像を幅の10%の範囲でランダムに左右に移動\n",
    "- **垂直移動 (`height_shift_range=0.1`)**: 画像を高さの10%の範囲でランダムに上下に移動\n",
    "- **水平反転 (`horizontal_flip=True`)**: 画像を左右反転（現実世界の多くの物体は左右反転しても同じクラスに属するため有効）\n",
    "\n",
    "これらの変換により、各エポックでモデルが見るトレーニングデータは毎回少しずつ異なります。これにより、モデルは画像の本質的な特徴に集中し、よりロバストな学習が可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルをコンパイル\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 過学習を防ぐためのデータ拡張を設定\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# データ拡張を使用してモデルを学習\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train_one_hot, batch_size=batch_size),\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test_one_hot),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6979c",
   "metadata": {},
   "source": [
    "### モデルのコンパイルと学習パラメータ\n",
    "\n",
    "#### オプティマイザ（Optimizer）\n",
    "\n",
    "- **Adam**: Adaptive Moment Estimationの略で、各パラメータに対して異なる学習率を自動調整します。\n",
    "  - **動作原理**: 勾配の1次モーメント（平均）と2次モーメント（分散）を利用して学習率を適応的に調整します。\n",
    "  - **利点**: 初期学習率にあまり敏感ではなく、勾配の鞍点や局所的最適解から抜け出しやすい性質があります。\n",
    "  - **学習率 (0.001)**: 大きすぎると収束しない、小さすぎると学習が遅くなるため、一般的に良いバランスとされる値を使用しています。\n",
    "\n",
    "#### 損失関数（Loss Function）\n",
    "\n",
    "- **カテゴリカルクロスエントロピー**: 多クラス分類タスクで標準的に使用される損失関数です。\n",
    "  - **数学的意味**: 予測確率分布と実際の分布（One-hotエンコード）の間の情報量の差を測定します。\n",
    "  - **数式**: $ -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i) $ （Cはクラス数、yは実際のクラス、ŷは予測確率）\n",
    "  - **最適値**: 値が小さいほど、モデルの予測がより正確であることを示します。\n",
    "\n",
    "#### バッチサイズとエポック\n",
    "\n",
    "- **バッチサイズ (64)**: 一度に処理する画像の数です。\n",
    "  - **大きいバッチサイズ**: 計算効率が良い、勾配の推定が安定する、より多くのGPUメモリが必要\n",
    "  - **小さいバッチサイズ**: ノイズの多い勾配だがそれが正則化効果を持つ場合もある、メモリ効率が良い\n",
    "\n",
    "- **エポック数 (25)**: 全トレーニングデータを何周学習するかを指定します。\n",
    "  - **少なすぎると**: モデルが十分に学習できない（過少学習）\n",
    "  - **多すぎると**: 過学習のリスクが高まる、計算コストが増加\n",
    "\n",
    "#### 検証データ\n",
    "\n",
    "学習中、各エポック終了時に検証データ（テストデータ）でモデルを評価します。これにより：\n",
    "- 過学習の早期検出が可能になる\n",
    "- モデルの汎化性能を確認できる\n",
    "- ハイパーパラメータの調整の指針となる\n",
    "\n",
    "検証データは学習に使用せず、モデルの性能評価のみに使用されることに注意が必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8eb866",
   "metadata": {},
   "source": [
    "## モデルの評価\n",
    "\n",
    "CNNの学習が完了したので、テストデータセットでその性能を評価してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでモデルを評価\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test_one_hot, verbose=2)\n",
    "print(f\"\\nテスト精度: {test_acc:.4f}\")\n",
    "print(f\"テスト損失: {test_loss:.4f}\")\n",
    "\n",
    "# 予測を実行\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.squeeze(y_test)\n",
    "\n",
    "# 分類レポート\n",
    "print(\"\\n分類レポート:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# 混同行列\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ebfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列をプロット\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('混同行列')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# テキスト注釈を追加\n",
    "thresh = conf_matrix.max() / 2\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        plt.text(j, i, conf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('正解ラベル')\n",
    "plt.xlabel('予測ラベル')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96a75b",
   "metadata": {},
   "source": [
    "## モデルのパフォーマンス可視化\n",
    "\n",
    "ここでは、エポックごとの訓練および検証の精度と損失をグラフ化して、モデルの学習性能を分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd849c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と検証の精度・損失をプロット\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 精度\n",
    "ax1.plot(history.history['accuracy'], label='訓練精度')\n",
    "ax1.plot(history.history['val_accuracy'], label='検証精度')\n",
    "ax1.set_title('モデルの精度')\n",
    "ax1.set_xlabel('エポック')\n",
    "ax1.set_ylabel('精度')\n",
    "ax1.legend()\n",
    "\n",
    "# 損失\n",
    "ax2.plot(history.history['loss'], label='訓練損失')\n",
    "ax2.plot(history.history['val_loss'], label='検証損失')\n",
    "ax2.set_title('モデルの損失')\n",
    "ax2.set_xlabel('エポック')\n",
    "ax2.set_ylabel('損失')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# モデルが過学習か、過少学習か、適切な学習かを分析\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "val_acc = history.history['val_accuracy'][-1]\n",
    "acc_diff = train_acc - val_acc\n",
    "\n",
    "if acc_diff > 0.1:\n",
    "    print(\"モデルが過学習の可能性あり - 訓練精度が検証精度より著しく高い\")\n",
    "elif train_acc < 0.7:\n",
    "    print(\"モデルが過少学習の可能性あり - 訓練精度と検証精度が共に低い\")\n",
    "else:\n",
    "    print(\"モデルは適切に学習できているようです\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e9528",
   "metadata": {},
   "source": [
    "## モデルでの予測\n",
    "\n",
    "ここでは、学習済みモデルを使用していくつかのテスト画像に対して予測を行い、結果を可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト画像のサブセットで予測を実行\n",
    "num_images = 15\n",
    "sample_indices = np.random.randint(0, len(x_test), num_images)\n",
    "sample_images = x_test[sample_indices]\n",
    "sample_labels = y_test[sample_indices]\n",
    "\n",
    "# 予測を取得\n",
    "predictions = model.predict(sample_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.squeeze(sample_labels)\n",
    "\n",
    "# 予測結果と一緒に画像をプロット\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.imshow(sample_images[i])\n",
    "    \n",
    "    color = 'green' if predicted_classes[i] == true_classes[i] else 'red'\n",
    "    title = f\"予測: {class_names[predicted_classes[i]]}\\n正解: {class_names[true_classes[i]]}\"\n",
    "    \n",
    "    plt.title(title, color=color)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf08cc5",
   "metadata": {},
   "source": [
    "## CNN活性化の可視化\n",
    "\n",
    "畳み込み層の活性化（特徴マップ）を観察することで、CNNがどのような特徴を検出しているのかを可視化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cf83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# First, select a sample image to visualize\n",
    "img_index = 12\n",
    "img = x_test[img_index:img_index+1]\n",
    "\n",
    "# Get all convolutional layers\n",
    "conv_layers = [layer for layer in model.layers if 'conv' in layer.name.lower()]\n",
    "\n",
    "# Create separate visualization models for each convolutional layer\n",
    "layer_activations = []\n",
    "for layer in conv_layers:\n",
    "    # Create a new model that returns just this layer's output\n",
    "    temp_model = Model(inputs=model.inputs, outputs=layer.output)\n",
    "    # Get activations for our sample image\n",
    "    activations = temp_model.predict(img)\n",
    "    layer_activations.append(activations)\n",
    "\n",
    "# 特定の層の活性化を表示する関数\n",
    "def display_activations(activations, layer_name, num_channels=16):\n",
    "    num_to_display = min(num_channels, activations.shape[-1])\n",
    "    display_grid = np.zeros((activations.shape[1], activations.shape[2] * num_to_display))\n",
    "    \n",
    "    for i in range(num_to_display):\n",
    "        # 各チャネルの活性化を取得\n",
    "        x = activations[0, :, :, i]\n",
    "        \n",
    "        # 可視化のために活性化を0から255の範囲にスケーリング\n",
    "        x -= x.mean()\n",
    "        x /= x.std() + 1e-5\n",
    "        x *= 64\n",
    "        x += 128\n",
    "        x = np.clip(x, 0, 255).astype('uint8')\n",
    "        \n",
    "        # チャネルをグリッドに追加\n",
    "        display_grid[:, i * activations.shape[2]:(i + 1) * activations.shape[2]] = x\n",
    "    \n",
    "    # グリッドをプロット\n",
    "    scale = 0.5\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    plt.title(f\"{layer_name}の特徴マップ\")\n",
    "    plt.show()\n",
    "\n",
    "# まず元の画像を表示\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(x_test[img_index])\n",
    "plt.title(f\"元の画像 - クラス: {class_names[y_test[img_index][0]]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 各畳み込み層の活性化を表示\n",
    "for i, activations in enumerate(layer_activations):\n",
    "    display_activations(activations, f\"畳み込み層 {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee01f07",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\n",
    "このノートブックでは、畳み込みニューラルネットワーク（CNN）について次のことを学びました：\n",
    "\n",
    "1. CIFAR-10データセットのロードと前処理\n",
    "2. 畳み込み層、プーリング層、全結合層を含むCNNアーキテクチャの構築\n",
    "3. 過学習を防ぐためのデータ拡張を用いたモデルの学習\n",
    "4. さまざまな指標を使用したモデルの性能評価\n",
    "5. モデルの学習進捗とその予測の可視化\n",
    "6. CNNが生成する特徴マップの探索\n",
    "\n",
    "CNNが画像分類タスクに強力な理由は以下の通りです：\n",
    "- 階層的特徴を自動的に学習する\n",
    "- 局所的な接続性により空間的な局所性を活用する\n",
    "- 画像全体でパラメータを共有する\n",
    "- プーリング操作により平行移動に不変である\n",
    "\n",
    "このモデルをさらに改善するには、以下の方法が考えられます：\n",
    "- 事前学習済みモデル（ResNet、VGGなど）を用いた転移学習の活用\n",
    "- より高度な正則化テクニックの適用\n",
    "- 異なるアーキテクチャとハイパーパラメータの実験\n",
    "- 収束性を向上させるための学習率スケジューラの使用"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
