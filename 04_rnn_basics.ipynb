{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e0dcb0",
   "metadata": {},
   "source": [
    "# 再帰型ニューラルネットワーク (RNN) 入門\n",
    "\n",
    "このノートブックでは、機械学習初心者向けに再帰型ニューラルネットワーク (RNN) の基本概念と実装方法を解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0f276",
   "metadata": {},
   "source": [
    "## なぜ再帰型ニューラルネットワーク（RNN）が必要なのか？\n",
    "\n",
    "### 通常のニューラルネットワークの制限\n",
    "\n",
    "従来の全結合（Fully Connected）ニューラルネットワークには、時系列データを扱う上で重要な制限があります：\n",
    "\n",
    "1. **入力と出力の長さが固定**: 柔軟な長さの系列データを扱えません\n",
    "2. **時間的な依存関係を無視**: データの順序情報やコンテキストを考慮できません\n",
    "3. **パラメータ共有なし**: 異なる位置で同じパターンを検出するために重みを共有できません\n",
    "\n",
    "例えば、文章「昨日は天気が良かった」を処理する場合、従来のニューラルネットワークでは単語同士の関連性や順序を適切に考慮できません。\n",
    "\n",
    "### RNNの革新性\n",
    "\n",
    "RNNは上記の問題を解決するために設計されています：\n",
    "\n",
    "1. **可変長の入出力を処理可能**: 単語数や時間ステップ数に関わらず柔軟に対応\n",
    "2. **時間的文脈を保持**: 「内部状態」を通じて過去の情報を記憶し活用\n",
    "3. **パラメータの共有**: 同じ変換関数を時系列全体で使用し、効率的に学習\n",
    "\n",
    "これにより、RNNはテキスト、音声、株価、センサーデータなどの順序や時間的文脈が重要なデータを扱うのに適しています。\n",
    "\n",
    "このノートブックを通じて、RNNの基本原理から実装、そして実際の時系列データ予測までをハンズオン形式で学びましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c349c7c",
   "metadata": {},
   "source": [
    "## ライブラリのインポート\n",
    "\n",
    "必要なライブラリ（NumPy、TensorFlow、Matplotlib など）をインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81875ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# バージョン確認\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# 結果の再現性のために乱数シードを設定\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90472233",
   "metadata": {},
   "source": [
    "## 再帰型ニューラルネットワーク (RNN) の概要\n",
    "\n",
    "再帰型ニューラルネットワーク（RNN）は、時系列データや順序データを処理するために設計された特殊なニューラルネットワークです。従来のニューラルネットワークとは異なり、RNNは内部状態（メモリ）を持ち、過去の入力情報を記憶して現在の出力に活用することができます。\n",
    "\n",
    "### RNNの主な応用分野:\n",
    "\n",
    "- **自然言語処理（NLP）**: テキスト生成、機械翻訳、感情分析など\n",
    "- **時系列予測**: 株価予測、天気予報、需要予測など\n",
    "- **音声認識**: 音声からテキストへの変換\n",
    "- **画像のキャプション生成**: 画像の内容を説明する文章の自動生成\n",
    "\n",
    "RNNの最大の特徴は「記憶」を持つことで、データの前後関係（コンテキスト）を理解できる点にあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b297a8",
   "metadata": {},
   "source": [
    "## RNN の基本構造\n",
    "\n",
    "RNNの基本構造は、入力層、隠れ層（再帰的な接続を持つ）、出力層から構成されています。\n",
    "\n",
    "### 数学的表現:\n",
    "\n",
    "時刻 $t$ における:\n",
    "- $x_t$: 入力\n",
    "- $h_t$: 隠れ状態\n",
    "- $y_t$: 出力\n",
    "- $W$, $U$, $V$: 重み行列\n",
    "- $b$, $c$: バイアス項\n",
    "\n",
    "以下の式で表されます:\n",
    "\n",
    "$$h_t = \\tanh(W \\cdot x_t + U \\cdot h_{t-1} + b)$$\n",
    "$$y_t = V \\cdot h_t + c$$\n",
    "\n",
    "### RNNの数式を詳しく理解する\n",
    "\n",
    "上記の式の各部分を分解して説明します：\n",
    "\n",
    "1. **隠れ状態の更新式** `$h_t = \\tanh(W \\cdot x_t + U \\cdot h_{t-1} + b)$`\n",
    "   - `$W \\cdot x_t$`: 現在の入力を変換（通常のニューラルネットワークと同様）\n",
    "   - `$U \\cdot h_{t-1}$`: 前の時間ステップの隠れ状態を変換（記憶の部分）\n",
    "   - `$b$`: バイアス項\n",
    "   - `$\\tanh$`: 活性化関数（-1から1の間に出力を収める）\n",
    "\n",
    "2. **出力の計算式** `$y_t = V \\cdot h_t + c$`\n",
    "   - `$V \\cdot h_t$`: 現在の隠れ状態を出力空間に変換\n",
    "   - `$c$`: 出力のバイアス項\n",
    "\n",
    "この計算の流れにより、RNNは時間的な文脈を維持しながら予測を行うことができます。前の時間ステップの情報（$h_{t-1}$）を常に考慮するため、時系列データの処理に適しています。\n",
    "\n",
    "### 図式表現:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1573de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import japanize_matplotlib\n",
    "import sys\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Check if running on Linux\n",
    "if sys.platform.startswith('linux'):\n",
    "    # List available fonts with Japanese support\n",
    "    fonts = [f for f in fm.findSystemFonts() if 'gothic' in f.lower() or 'mincho' in f.lower() or 'meiryo' in f.lower()]\n",
    "    \n",
    "    if fonts:\n",
    "        # Use the first available Japanese font\n",
    "        plt.rcParams['font.family'] = fm.FontProperties(fname=fonts[0]).get_name()\n",
    "    else:\n",
    "        # Fallback to IPAGothic or another common Japanese font\n",
    "        plt.rcParams['font.family'] = 'IPAGothic, Noto Sans CJK JP, MS Gothic'\n",
    "else:\n",
    "    # On Windows/Mac, use platform-specific fonts\n",
    "    if sys.platform.startswith('win'):\n",
    "        plt.rcParams['font.family'] = 'MS Gothic'\n",
    "    elif sys.platform.startswith('darwin'):\n",
    "        plt.rcParams['font.family'] = 'AppleGothic'\n",
    "\n",
    "# Disable font warnings (optional)\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "japanize_matplotlib.japanize()  # Apply japanize_matplotlib settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNの図式表現\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
    "\n",
    "# RNN構造の図を作成\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 隠れ状態のボックスを描画\n",
    "h_boxes = []\n",
    "for i in range(4):\n",
    "    h_box = Rectangle((i*2, 1), 1, 1, fc='lightblue', ec='black', alpha=0.7)\n",
    "    ax.add_patch(h_box)\n",
    "    h_boxes.append(h_box)\n",
    "    ax.text(i*2 + 0.5, 1.5, f\"h{i}\", ha='center', va='center', fontsize=14)\n",
    "\n",
    "# 入力のボックスを描画\n",
    "for i in range(4):\n",
    "    x_box = Rectangle((i*2, 0), 1, 0.6, fc='lightgreen', ec='black', alpha=0.7)\n",
    "    ax.add_patch(x_box)\n",
    "    ax.text(i*2 + 0.5, 0.3, f\"x{i}\", ha='center', va='center', fontsize=14)\n",
    "\n",
    "# 出力のボックスを描画\n",
    "for i in range(4):\n",
    "    y_box = Rectangle((i*2, 2.5), 1, 0.6, fc='salmon', ec='black', alpha=0.7)\n",
    "    ax.add_patch(y_box)\n",
    "    ax.text(i*2 + 0.5, 2.8, f\"y{i}\", ha='center', va='center', fontsize=14)\n",
    "\n",
    "# 接続を描画\n",
    "for i in range(3):\n",
    "    # 隠れ状態から次の隠れ状態への接続\n",
    "    arrow = FancyArrowPatch((i*2 + 1, 1.5), ((i+1)*2, 1.5), \n",
    "                          connectionstyle=\"arc3,rad=0.1\", \n",
    "                          arrowstyle='->', \n",
    "                          mutation_scale=15, \n",
    "                          lw=1.5, \n",
    "                          color='blue')\n",
    "    ax.add_patch(arrow)\n",
    "\n",
    "# 入力から隠れ状態への接続と隠れ状態から出力への接続\n",
    "for i in range(4):\n",
    "    # 入力から隠れ状態への接続\n",
    "    arrow_in = FancyArrowPatch((i*2 + 0.5, 0.6), (i*2 + 0.5, 1), \n",
    "                             arrowstyle='->', \n",
    "                             mutation_scale=15, \n",
    "                             lw=1.5, \n",
    "                             color='green')\n",
    "    ax.add_patch(arrow_in)\n",
    "    \n",
    "    # 隠れ状態から出力への接続\n",
    "    arrow_out = FancyArrowPatch((i*2 + 0.5, 2), (i*2 + 0.5, 2.5), \n",
    "                              arrowstyle='->', \n",
    "                              mutation_scale=15, \n",
    "                              lw=1.5, \n",
    "                              color='red')\n",
    "    ax.add_patch(arrow_out)\n",
    "\n",
    "ax.set_xlim(-0.5, 8.5)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "ax.set_title('RNNの基本構造', fontsize=16)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf37a05",
   "metadata": {},
   "source": [
    "## サンプルデータの準備\n",
    "\n",
    "RNNモデルをテストするために、簡単な時系列データを生成します。ここでは、正弦波のデータを作成し、そのパターンを学習させることを目標とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071141e",
   "metadata": {},
   "source": [
    "### 時系列データとRNNのための前処理方法\n",
    "\n",
    "時系列データをRNNで扱うには、特定のフォーマットに変換する必要があります。一般的な手法は「スライディングウィンドウ」（時間窓）アプローチです。\n",
    "\n",
    "**スライディングウィンドウの考え方:**\n",
    "\n",
    "![スライディングウィンドウの図](https://i.imgur.com/WCbqnBD.png)\n",
    "\n",
    "1. **入力シーケンス長を決定**: 何個の過去のデータポイントを使って将来を予測するか（例：過去20時点）\n",
    "2. **ウィンドウをスライド**: 一定間隔でウィンドウを移動させながらサンプルを作成\n",
    "3. **入力と出力のペア作成**: \n",
    "   - **入力 X**: [時点0, 時点1, ..., 時点19]\n",
    "   - **出力 y**: 時点20\n",
    "\n",
    "**RNN用データの次元形状:**\n",
    "\n",
    "Kerasの`SimpleRNN`層は以下の形状の入力を期待します：\n",
    "\n",
    "`[サンプル数, 時間ステップ数, 特徴量の数]`\n",
    "\n",
    "- **サンプル数**: データのバッチサイズまたは全体のサンプル数\n",
    "- **時間ステップ数**: 1つのサンプルに含まれる時点の数（過去何時点を見るか）\n",
    "- **特徴量の数**: 各時点で観測される変数の数（単変量なら1, 多変量なら複数）\n",
    "\n",
    "次のコードで実際に時系列データを生成し、RNN用に変換する方法を見ていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列データの生成（正弦波）\n",
    "def generate_sine_wave(sample_size=1000, time_steps=50, frequency=0.1, noise=0.1):\n",
    "    \"\"\"正弦波の時系列データを生成する関数\"\"\"\n",
    "    x = np.linspace(0, sample_size * frequency, sample_size)\n",
    "    y = np.sin(x)\n",
    "    \n",
    "    # ノイズを追加\n",
    "    if noise > 0:\n",
    "        y += np.random.normal(0, noise, size=len(y))\n",
    "    \n",
    "    # データをスケーリング（-1から1の範囲に）\n",
    "    y = y / np.max(np.abs(y))\n",
    "    \n",
    "    # 訓練データの準備（時系列データをスライディングウィンドウ形式に変換）\n",
    "    X, Y = [], []\n",
    "    for i in range(len(y) - time_steps):\n",
    "        X.append(y[i:i + time_steps])\n",
    "        Y.append(y[i + time_steps])\n",
    "    \n",
    "    # numpy配列に変換\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    # RNN入力形式に変形 [samples, time_steps, features]\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# データの生成\n",
    "time_steps = 20  # 過去20ポイントを使用して次のポイントを予測\n",
    "X_train, y_train = generate_sine_wave(sample_size=1000, time_steps=time_steps, frequency=0.05, noise=0.05)\n",
    "X_test, y_test = generate_sine_wave(sample_size=200, time_steps=time_steps, frequency=0.05, noise=0.05)\n",
    "\n",
    "print(f\"訓練データの形状: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"テストデータの形状: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# データの可視化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_train[:200])\n",
    "plt.title('生成した正弦波データのサンプル', fontsize=14)\n",
    "plt.xlabel('時間', fontsize=12)\n",
    "plt.ylabel('振幅', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f88ab",
   "metadata": {},
   "source": [
    "### 生成したデータの解説\n",
    "\n",
    "上記のコードでは次のことを行いました：\n",
    "\n",
    "1. **シンプルな正弦波の生成**:\n",
    "   - 周波数0.05の正弦波を生成（ゆっくりとした波形）\n",
    "   - ガウスノイズを追加して実際のデータに近づける（ノイズ強度0.05）\n",
    "\n",
    "2. **データの正規化**:\n",
    "   - データの振幅を-1〜1の範囲に正規化\n",
    "   - これにより訓練が安定し、勾配消失/爆発問題を回避\n",
    "\n",
    "3. **スライディングウィンドウ変換**:\n",
    "   - 入力: 連続する20時点のデータ\n",
    "   - 出力: 21時点目のデータ（次の値）\n",
    "\n",
    "4. **RNN入力形式への変換**:\n",
    "   - 3次元配列形式： [サンプル数, 時間ステップ数, 特徴量の数]\n",
    "   - X_train.shape = (980, 20, 1) → 980個のサンプル、各サンプルは20時点分のデータ、各時点の特徴量は1\n",
    "\n",
    "このようにして準備されたデータは、RNNモデルで時系列予測タスクを学習するのに適した形式になっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7eeba",
   "metadata": {},
   "source": [
    "## 単純な RNN モデルの実装\n",
    "\n",
    "TensorFlow/Kerasを使用して単純なRNNモデルを実装します。以下のステップに従います：\n",
    "\n",
    "1. モデルのアーキテクチャ設計\n",
    "2. モデルのコンパイル\n",
    "3. モデルの概要確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502708d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単純なRNNモデルの構築\n",
    "model = Sequential([\n",
    "    SimpleRNN(units=64, activation='tanh', return_sequences=False, input_shape=(time_steps, 1)),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# モデルの概要\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394e362",
   "metadata": {},
   "source": [
    "### モデルの解説\n",
    "\n",
    "上記で構築したRNNモデルは以下の層で構成されています：\n",
    "\n",
    "1. **SimpleRNN層**：\n",
    "   - 64ユニット（隠れ層のサイズ）\n",
    "   - tanh活性化関数（-1から1の範囲で出力）\n",
    "   - return_sequences=False（最後の時間ステップの出力のみを次の層に渡す）\n",
    "   - input_shape=(time_steps, 1)：入力データ形状（時間ステップ数, 特徴量の数）\n",
    "\n",
    "2. **全結合層（Dense）**：\n",
    "   - 32ユニット\n",
    "   - ReLU活性化関数\n",
    "\n",
    "3. **出力層**：\n",
    "   - 1ユニット（1つの値を予測）\n",
    "\n",
    "このモデルは、過去20時点のデータから次の1時点の値を予測するように設計されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed44b4",
   "metadata": {},
   "source": [
    "## モデルのトレーニングと評価\n",
    "\n",
    "構築したRNNモデルをトレーニングし、その性能を評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのトレーニング\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 訓練履歴の可視化\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='トレーニング損失')\n",
    "plt.plot(history.history['val_loss'], label='検証損失')\n",
    "plt.title('モデルの学習曲線', fontsize=14)\n",
    "plt.xlabel('エポック', fontsize=12)\n",
    "plt.ylabel('損失（MSE）', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# テストデータでモデルを評価\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"テストデータでの損失（MSE）: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f6e71",
   "metadata": {},
   "source": [
    "## モデル評価の指標と解釈\n",
    "\n",
    "### 時系列予測の評価方法\n",
    "\n",
    "時系列予測モデルの性能を評価する際には、複数の指標と視覚化手法を用いることが重要です。以下では、一般的な評価指標とその解釈方法について説明します：\n",
    "\n",
    "#### 1. 主要な評価指標\n",
    "\n",
    "- **平均二乗誤差 (MSE)**：最も基本的な指標で、予測値と実際値の差の二乗の平均\n",
    "  - 値が小さいほど良いモデル\n",
    "  - 外れ値に敏感（二乗するため）\n",
    "  - 数式：$\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "- **平均絶対誤差 (MAE)**：予測値と実際値の差の絶対値の平均\n",
    "  - こちらも値が小さいほど良いモデル\n",
    "  - MSEよりも外れ値の影響を受けにくい\n",
    "  - 数式：$\\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$\n",
    "\n",
    "- **平均絶対パーセント誤差 (MAPE)**：相対的な誤差を示し、異なるスケールのデータセット間で比較可能\n",
    "  - 数式：$\\frac{100\\%}{n}\\sum_{i=1}^{n}|\\frac{y_i - \\hat{y}_i}{y_i}|$\n",
    "\n",
    "#### 2. 視覚化による評価\n",
    "\n",
    "- **実際値vs予測値プロット**：時系列に沿って実際値と予測値を重ねて表示し、モデルがパターンをどれだけ捉えているか確認\n",
    "- **散布図**：実際値（x軸）と予測値（y軸）の関係を表示。完全な予測では45度線上に点が並ぶ\n",
    "- **残差（誤差）プロット**：予測誤差の時間的パターンを確認。理想的には誤差はランダムで構造を持たない\n",
    "- **誤差分布**：誤差のヒストグラム。理想的には平均0の正規分布に近い形状\n",
    "\n",
    "#### 3. 交差検証の重要性\n",
    "\n",
    "時系列データの交差検証は通常のデータとは少し異なります：\n",
    "- **時間的依存性を考慮**：将来のデータで過去を予測することはできない\n",
    "- **時間順序を維持**：ランダムな分割ではなく、時間順序に沿った分割が必要\n",
    "- **タイムシリーズクロスバリデーション**：時間の経過と共に拡大するトレーニングセットと、それに続くテストセット\n",
    "\n",
    "この後のコードでは、これらの評価指標と視覚化手法を用いて、RNNモデルの性能を詳細に分析します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d5fe5",
   "metadata": {},
   "source": [
    "## 予測の可視化\n",
    "\n",
    "トレーニングしたRNNモデルを使用して予測を行い、結果を視覚的に確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d05b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの予測\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 予測結果のプロット（最初の100ポイント）\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test[:100], 'b-', label='実際の値', alpha=0.6)\n",
    "plt.plot(y_pred[:100], 'r-', label='予測値', alpha=0.6)\n",
    "plt.title('RNNモデルによる時系列予測', fontsize=14)\n",
    "plt.xlabel('時間ステップ', fontsize=12)\n",
    "plt.ylabel('値', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 予測と実際の値の散布図\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([-1, 1], [-1, 1], 'r--')  # 完全一致線\n",
    "plt.title('予測値 vs 実際の値', fontsize=14)\n",
    "plt.xlabel('実際の値', fontsize=12)\n",
    "plt.ylabel('予測値', fontsize=12)\n",
    "plt.xlim([-1.1, 1.1])\n",
    "plt.ylim([-1.1, 1.1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 誤差分布\n",
    "errors = y_pred.flatten() - y_test\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(errors, bins=30, alpha=0.7)\n",
    "plt.title('予測誤差の分布', fontsize=14)\n",
    "plt.xlabel('誤差', fontsize=12)\n",
    "plt.ylabel('頻度', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 統計量の表示\n",
    "print(f\"予測誤差の平均: {np.mean(errors):.6f}\")\n",
    "print(f\"予測誤差の標準偏差: {np.std(errors):.6f}\")\n",
    "print(f\"平均絶対誤差 (MAE): {np.mean(np.abs(errors)):.6f}\")\n",
    "print(f\"平均二乗誤差 (MSE): {np.mean(errors**2):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80619b",
   "metadata": {},
   "source": [
    "## LSTMと比較する\n",
    "\n",
    "SimpleRNNは長期的な依存関係を学習するのが難しいことがあります（勾配消失問題）。代わりにLSTM（Long Short-Term Memory）を使用して同じタスクに取り組み、結果を比較してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5178be31",
   "metadata": {},
   "source": [
    "### LSTMの基本構造：SimpleRNNの限界を克服する\n",
    "\n",
    "**勾配消失問題：SimpleRNNの主要な課題**\n",
    "\n",
    "SimpleRNNでは、時間ステップが進むにつれて勾配（誤差の伝播）が指数関数的に小さくなる「勾配消失問題」が発生します。これにより：\n",
    "- 長期的な依存関係（前の時間ステップの情報）が学習されにくい\n",
    "- モデルが最近の情報に過度に依存する\n",
    "- 複雑なパターンの認識や将来予測の精度が低下する\n",
    "\n",
    "**LSTMの主要コンポーネント**\n",
    "\n",
    "LSTM（Long Short-Term Memory）は、この問題を解決するために特別に設計されました。主要な構成要素は：\n",
    "\n",
    "1. **セルステート（$C_t$）**：長期記憶を保持する経路\n",
    "2. **3つのゲート機構**：\n",
    "   - **忘却ゲート（$f_t$）**：古い情報を忘れるかどうかを制御\n",
    "   - **入力ゲート（$i_t$）**：新しい情報をどれだけ保存するかを制御\n",
    "   - **出力ゲート（$o_t$）**：現在のセルステートからどれだけ出力するかを制御\n",
    "\n",
    "**LSTMの数学的表現（簡略化）**\n",
    "\n",
    "```\n",
    "f_t = σ(Wf·[h_{t-1}, x_t] + bf)  # 忘却ゲート\n",
    "i_t = σ(Wi·[h_{t-1}, x_t] + bi)  # 入力ゲート\n",
    "C̃_t = tanh(Wc·[h_{t-1}, x_t] + bc)  # 候補となるセルステート\n",
    "C_t = f_t * C_{t-1} + i_t * C̃_t  # セルステートの更新\n",
    "o_t = σ(Wo·[h_{t-1}, x_t] + bo)  # 出力ゲート\n",
    "h_t = o_t * tanh(C_t)  # 隠れ状態の出力\n",
    "```\n",
    "\n",
    "この複雑な構造により、LSTMは：\n",
    "- 長期的な依存関係を効果的に学習できる\n",
    "- 関連性の高い情報を長期間保持できる\n",
    "- 勾配消失問題を緩和できる\n",
    "\n",
    "これからLSTMモデルを実装し、SimpleRNNと性能を比較してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMモデルの構築\n",
    "lstm_model = Sequential([\n",
    "    LSTM(units=64, activation='tanh', return_sequences=False, input_shape=(time_steps, 1)),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=1)\n",
    "])\n",
    "\n",
    "# モデルのコンパイル\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# モデルの訓練\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# テストデータでの予測\n",
    "lstm_y_pred = lstm_model.predict(X_test)\n",
    "\n",
    "# RNNとLSTMの比較\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 損失の比較\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['val_loss'], 'b-', label='SimpleRNN')\n",
    "plt.plot(lstm_history.history['val_loss'], 'r-', label='LSTM')\n",
    "plt.title('SimpleRNN vs LSTM: 検証損失', fontsize=14)\n",
    "plt.xlabel('エポック', fontsize=12)\n",
    "plt.ylabel('損失（MSE）', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 予測の比較\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y_test[:100], 'k-', label='実際の値', alpha=0.7)\n",
    "plt.plot(y_pred[:100], 'b-', label='SimpleRNN', alpha=0.6)\n",
    "plt.plot(lstm_y_pred[:100], 'r-', label='LSTM', alpha=0.6)\n",
    "plt.title('SimpleRNN vs LSTM: 予測結果', fontsize=14)\n",
    "plt.xlabel('時間ステップ', fontsize=12)\n",
    "plt.ylabel('値', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 両モデルのテスト損失の比較\n",
    "rnn_test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "lstm_test_loss = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"SimpleRNNのテスト損失（MSE）: {rnn_test_loss:.6f}\")\n",
    "print(f\"LSTMのテスト損失（MSE）: {lstm_test_loss:.6f}\")\n",
    "print(f\"改善率: {((rnn_test_loss - lstm_test_loss) / rnn_test_loss) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea078c",
   "metadata": {},
   "source": [
    "## 実務での応用例\n",
    "\n",
    "RNNとLSTMは様々な時系列データや順序データの分析・予測に使われています。実際の応用例をいくつか見てみましょう：\n",
    "\n",
    "### 1. 金融工学\n",
    "\n",
    "- **株価予測**：過去の株価データから将来のトレンドを予測\n",
    "- **異常検出**：不正取引や異常な市場動向の検出\n",
    "- **リスク評価**：将来のボラティリティやリスク水準の予測\n",
    "\n",
    "### 2. 自然言語処理（NLP）\n",
    "\n",
    "- **文章生成**：次の単語や文を予測・生成\n",
    "- **感情分析**：テキストの感情や意図を認識\n",
    "- **機械翻訳**：ある言語から別の言語へのテキスト変換\n",
    "\n",
    "### 3. 時系列センサーデータ分析\n",
    "\n",
    "- **工場の機器故障予測**：センサーデータから異常や故障の予兆を検知\n",
    "- **エネルギー消費予測**：過去のパターンから将来のエネルギー需要を予測\n",
    "- **気象予報**：気象データからの天候変化予測\n",
    "\n",
    "### 4. 医療データ分析\n",
    "\n",
    "- **生体信号処理**：心拍データや脳波データからの異常検出\n",
    "- **病状の進行予測**：患者データから疾患の進行パターンを予測\n",
    "- **医療画像の時系列解析**：MRIやCTスキャンの時系列変化の分析\n",
    "\n",
    "### 応用のための考慮点\n",
    "\n",
    "実際の応用では、以下の点を考慮することが重要です：\n",
    "\n",
    "1. **データの前処理**：欠損値、外れ値、正規化など適切な前処理が結果を大きく左右\n",
    "2. **特徴量エンジニアリング**：原データから有用な特徴を抽出することで予測精度が向上\n",
    "3. **モデル選択**：タスクの複雑さによってSimple RNN、LSTM、GRU、Bidirectional RNNなど適切なモデルを選択\n",
    "4. **ハイパーパラメータ調整**：層の数、ユニット数、学習率などを最適化\n",
    "5. **評価方法**：適切な評価指標と検証方法の選択\n",
    "\n",
    "このノートブックで学んだ基礎知識をもとに、実際の問題に取り組む際はこれらの点を考慮してモデルを構築しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127de7c0",
   "metadata": {},
   "source": [
    "## 発展的なRNNアーキテクチャ\n",
    "\n",
    "RNNには基本的なSimple RNNとLSTM以外にも、様々な発展形があります。それぞれの特徴と利点について簡単に紹介します：\n",
    "\n",
    "### 1. GRU（Gated Recurrent Unit）\n",
    "\n",
    "LSTMの簡略版で、計算効率が良く、少ないパラメータ数でも高い性能を発揮します。\n",
    "\n",
    "- **特徴**: LSTMの4つのゲートを2つ（更新ゲートとリセットゲート）に削減\n",
    "- **利点**: 計算量が少なく、訓練が高速で、パラメータ数が少ないためデータが少ない場合も効果的\n",
    "- **適用例**: 中短期の依存関係を持つシーケンスデータの処理\n",
    "\n",
    "### 2. 双方向RNN（Bidirectional RNN）\n",
    "\n",
    "過去の情報だけでなく、未来の情報も考慮したい場合に使用します。\n",
    "\n",
    "- **特徴**: 2つのRNN（順方向と逆方向）を組み合わせて、過去と未来の両方のコンテキストを考慮\n",
    "- **利点**: より豊富なコンテキスト情報を活用でき、特に文章理解などの領域で効果的\n",
    "- **適用例**: 単語の品詞タグ付け、センチメント分析、テキスト分類\n",
    "\n",
    "### 3. スタックRNN（Stacked/Deep RNN）\n",
    "\n",
    "複数のRNN層を積み重ねることで、より複雑なパターンを学習できます。\n",
    "\n",
    "- **特徴**: RNN層を複数層重ねる構造\n",
    "- **利点**: より複雑なパターンや階層的な特徴を学習可能\n",
    "- **適用例**: 複雑な時系列パターンの認識、自然言語の意味理解\n",
    "\n",
    "### 4. Attention機構を持つRNN\n",
    "\n",
    "長いシーケンスデータ処理での情報損失を防ぎ、関連性の高い部分に「注意」を向けます。\n",
    "\n",
    "- **特徴**: 入力シーケンスの各位置に重み付けして、関連性の高い部分を強調\n",
    "- **利点**: 長いシーケンスでも情報損失が少なく、解釈可能性が向上\n",
    "- **適用例**: 機械翻訳、画像キャプション生成、長文の要約\n",
    "\n",
    "### 5. Transformer（自己注意機構）\n",
    "\n",
    "RNNの拡張というよりは代替モデルですが、時系列データ処理の最新技術として重要です。\n",
    "\n",
    "- **特徴**: RNNのような逐次処理を行わず、自己注意機構（Self-Attention）でシーケンス全体を並列処理\n",
    "- **利点**: 長距離依存関係の学習が容易で、並列処理による高速化が可能\n",
    "- **適用例**: 大規模言語モデル（BERT、GPT等）、高精度な時系列予測\n",
    "\n",
    "これらの発展的なモデルを理解し活用することで、より複雑な時系列問題にも対応できるようになります。まずはこのノートブックで学んだ基本的なRNNとLSTMをしっかり理解し、その後必要に応じて高度なモデルへと進んでいくことをお勧めします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660907ae",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、再帰型ニューラルネットワーク（RNN）の基本的な概念と実装方法を学びました。特に以下のポイントを理解できたはずです：\n",
    "\n",
    "1. **RNNの基本構造**: 入力層、再帰的な隠れ層、出力層からなる構造と、その数学的表現\n",
    "2. **時系列データの準備**: 時系列データをRNNモデル用に適切な形式に変換する方法\n",
    "3. **RNNモデルの実装**: TensorFlow/Kerasを使ってSimpleRNNモデルを構築する方法\n",
    "4. **モデルの評価**: 訓練済みモデルの性能を異なる視点から評価する方法\n",
    "5. **LSTMとの比較**: 従来のRNNとLSTMの性能差\n",
    "\n",
    "### 改善の余地\n",
    "\n",
    "- より複雑なデータセットでの実験\n",
    "- ハイパーパラメータのチューニング\n",
    "- 双方向RNN（Bidirectional RNN）や注意機構（Attention mechanism）の導入\n",
    "- さまざまな時系列予測タスクへの応用\n",
    "\n",
    "RNNは時系列データや順序データを扱う上で強力なツールですが、特に長期的な依存関係を学習する場合はLSTMやGRUなどの派生アーキテクチャがより効果的であることが多いです。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
